# -*- coding: utf-8 -*-
"""cis5450_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XAHOCAkL6DlwLDuBe7N3ra1YQ3ufBoeo

# Brazilian E-Commerce Product Rating Prediction
**Xiangchen Guan, Yicheng Shen, Zhenyuan Wang** <br/>
UPENN - CIS5450 Final Project <br/>
DEC 11, 2022

## **Section 0: Introduction**

### **Abstract**

For this project, we analyze the Brazilian E-Commerce Public Dataset by Olist dataset and build predictive models, which can help companies to make more accurate and informed decisions about their business and operations. Our contributions include: i) a detailed exploratory data analysis, including correlations, geospacial analysis, textual analysis, etc., ii) a number of new features that combine or transform the original raw data to prepare for modeling, and iii) a number of predictive models, such as regression models, XGBoost, LSTM neural network, etc. In our evaluation, most of our models perform better than the baseline and a few achieve close to human-level performance. With the batch processing capability of our models, our results have the potential to help sellers in practice to improve their customer ratings and thus improve revenue and profitability in the long run.

### **Motivation**

In recent years, E-commerce has become increasingly important, as more and more people are using the internet to shop online and conduct other business transactions. The growth of e-commerce has been driven by several factors, such as the increasing availability and affordability of internet access, the proliferation of mobile devices, and the development of new technologies and payment systems.

The importance of e-commerce is evident in many ways. For example, e-commerce has made it possible for people to shop for a wide variety of products and services from the convenience of their own homes, and it has enabled companies to reach a global market and sell their products to customers around the world. E-commerce has also had a major impact on traditional brick-and-mortar retail, as many companies have had to adapt to the rise of online shopping or risk losing market share to their online competitors.

Most importantly, e-commerce is an important and growing sector that has had a significant impact on the global economy and the way people shop and conduct business. It is likely to continue to play a key role in the future, as more and more people turn to the internet for their shopping and business needs.

Therefore, we believe studying and analyzing an E-commerce dataset is a meaningful task and its results can potentially provide valuable insights and information that can help companies to improve their business and operations.

We list some benefits that motivated us to work on this task:

**Understanding consumer behavior:** Studying an E-commerce dataset can provide valuable insights into consumer behavior, such as what products are popular, how much people are willing to pay for different products, and how customer preferences and purchasing patterns vary over time. This information can be useful for e-commerce companies and online marketplaces, as it can help them understand their customers better and make more informed decisions about their business.

**Improving the customer experience:** By analyzing E-commerce datasets, companies can identify areas where the customer experience can be improved, such as by addressing common customer complaints or identifying opportunities for upselling and cross-selling. This can help companies to improve customer satisfaction and retain more customers over the long term.

**Identifying trends and patterns:** E-commerce datasets can also be used to identify trends and patterns in the data, such as changes in consumer behavior over time or correlations between different variables. Such information can help companies to anticipate changes in the market and adapt their business strategies accordingly.

**Developing predictive models:** Finally, e-commerce datasets can be used to develop predictive models that can forecast future trends and behavior, such as predicting customer churn or estimating the demand for a particular product. In addition, by using predictive models to forecast demand and adjust supply accordingly, companies can improve their revenue and profitability.  These predictive models can help companies to make more accurate and informed decisions about their business and operations.

### **Dataset Summary**

Dataset: [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

We use the Brazilian E-Commerce Public Dataset by Olist dataset from Kaggle.

> The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers.

This dataset consists of several different tables that contain information about different aspects of the e-commerce transactions. For example, the `order_items` table contains information about the items that were purchased in each order, such as the product name, price, and quantity. The `orders` table contains information about the orders themselves, such as the order date, payment method, and shipping information. The `customers` table contains information about the customers who placed the orders, such as their name, email, and address. In addition to these tables, the dataset also includes tables that contain information about the products, sellers, and categories in the Olist marketplace. These tables can be used to study the products and sellers on the platform, as well as the relationships between different products and categories.

Overall, it contains a large amount of high-quality data about orders, customers, and products from a real-world e-commerce platform, and it can be used for a variety of different analyses.

## **Section 1: Environment Setup**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Install dependencies**"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install opendatasets
# !pip install kaggle
# 
# # Translator
# !pip3 install googletrans==3.1.0a0
# 
# # geo files for Brazil
# !pip install geobr
# !pip install mapsmx
# !pip install xgboost

"""**Import libaries**"""

import os
import time
import pandas as pd
import numpy as np
import re
import string
from scipy import stats
from random import sample
from collections import Counter
from PIL import Image

# Plotting Libraries
from matplotlib import pyplot as plt
from matplotlib import cm
import seaborn as sns
sns.set_theme(context="notebook", style="darkgrid", palette="pastel")

# Geospacial Libraries
import geobr
import mapsmx as mp
import plotly.express as px 
import geopandas as gpd

# NLP Libraries
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from googletrans import Translator
from wordcloud import WordCloud

# sklearn
from sklearn.model_selection import train_test_split
import sklearn.linear_model as lm
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.cluster import KMeans

# torch
import torch
import torchvision
from torchvision import transforms, utils
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.io import read_image
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# XGBoost
import xgboost as xgb

"""**Set seed for random operations**"""

seed = 42

"""## **Section 2: Download and load data**

Upload `kaggle.json` (containing your Kaggle credentials) to download Kaggle datasets.
"""

from google.colab import files
# files.upload()

"""Or if you already have `kaggle.json` file downloaded under `/content/drive/MyDrive`, you can ignore the step above.

Download dataset
"""

! rm -r ~/.kaggle
! mkdir -p ~/.kaggle
# ! cp kaggle.json ~/.kaggle
! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download -d olistbr/brazilian-ecommerce
! mkdir -p /content/raw_data
! unzip -o brazilian-ecommerce.zip -d /content/raw_data
! rm -f brazilian-ecommerce.zip

customers_df = pd.read_csv("./raw_data/olist_customers_dataset.csv")
geolocation_df = pd.read_csv("./raw_data/olist_geolocation_dataset.csv")
order_items_df = pd.read_csv("./raw_data/olist_order_items_dataset.csv")
order_payments_df = pd.read_csv("./raw_data/olist_order_payments_dataset.csv")
order_reviews_df = pd.read_csv("./raw_data/olist_order_reviews_dataset.csv")
orders_df = pd.read_csv("./raw_data/olist_orders_dataset.csv")
products_df = pd.read_csv("./raw_data/olist_products_dataset.csv")
sellers_df = pd.read_csv("./raw_data/olist_sellers_dataset.csv")
product_category_name_translation_df = pd.read_csv("./raw_data/product_category_name_translation.csv")

"""## **Section 3: Data Wrangling**

### **3.1. Joining Tables Together**

Data are stored in a number of tables, such as the orders table, products table, reviews table, etc. The data are stored in this manner because multiple tables enhance human readability and reduces potential duplicated data. Nevertheless, this format does not allow us to look at data in multiple tables together simultaneously. It is more convininent if we join these tables together before we proceed with our analysis. We join the tables according to the data schema shown below.

**Data Schema**
![schema](https://i.imgur.com/HRhd2Y0.png) Source: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
"""

# Merge data stored in various tables
full_df = orders_df.merge(order_reviews_df, on='order_id')\
                   .merge(order_payments_df, on='order_id')\
                   .merge(customers_df, on='customer_id')\
                   .merge(order_items_df, on='order_id')\
                   .merge(products_df, on='product_id')\
                   .merge(sellers_df, on='seller_id')
# Drop duplicates
full_df = full_df.drop_duplicates()

"""### **3.2. DataFrame Summary**

We use the `df_summary` function to inspect our data at a high level. It is more informative and readable than the `DataFrame.info()` function provided by Pandas. We will also use this to check our data after preprocessing.
"""

def df_summary(df):
    print(f"Dataset Shape: {df.shape}")
    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])
    summary = summary.reset_index()
    summary['Column'] = summary['index']
    summary = summary[['Column','dtypes']]
    summary['# Missing'] = df.isnull().sum().values    
    summary['# Unique'] = df.nunique().values
    summary['Example'] = df.loc[0].values

    for name in summary['Column'].value_counts().index:
        summary.loc[summary['Column'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) 

    return summary
    
df_summary(full_df)

"""### **3.3. Data Types Conversion**"""

datetime_columns = ['order_purchase_timestamp', 'order_approved_at', 
                    'order_delivered_carrier_date', 'order_delivered_customer_date', 
                    'order_estimated_delivery_date', 
                    'review_creation_date', 'review_answer_timestamp',
                    'shipping_limit_date']
full_df[datetime_columns] = full_df[datetime_columns].apply(pd.to_datetime)

"""### **3.4. Fix Improper Column Names**"""

full_df = full_df.rename(columns={'product_name_lenght': 'product_name_length',
                                   'product_description_lenght': 'product_description_length'
                                  })

"""After all the preprocessing, let's check the summary of the data again."""

df_summary(full_df)

"""## **Section 4: Exploratory Data Analysis (EDA)**

EDA is an important step before building models. EDA helps us understand the characteristics of the dataset and identify any anomalies or peculiarities that may affect modelling. EDA can also help us spot the distribution of data and identify potential trends or patterns that may be useful for building models. By performing EDA, we gain a better understanding of the data, which we use as insights and clues that direct us to proceed building our models.

### **4.1. Correlations**

First of all, we find correlation in our dataset. This step is important because it can help us identify relationships between different variables.  Correlated variables can help us reason about potential causes and effects.

The data contains columns of different types. For correlation analysis, we focus on the numeric columns. So, we need to first filter out the numeric columns.
"""

numeric_columns = ['review_score', 'payment_sequential', 'payment_installments',
                   'payment_value', 'price', 'freight_value', 'product_name_length',
                   'product_description_length', 'product_photos_qty', 'product_weight_g',
                   'product_length_cm', 'product_height_cm', 'product_width_cm']
numeric_df = full_df[numeric_columns]
corr = numeric_df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, cmap='RdBu', norm=plt.Normalize(-1,1), annot=True, fmt='.2f')
plt.title("Correlation Heatmap")
plt.show()

"""#### **4.1.1. Correlation Analysis of Payment Value**
We use thresholds of 0.3 and -0.3 to filter out the features that are most correlated with the payment value. As shown below, the price, freight value (transportation and delivery cost), and product weight are most correlated with the payment value. This result aligns with our common sense: the price and delivery cost are clearly main components of the payment; the weight of the product is a main determinent of the transportation cost. Therefore, the change in these features could largely affect the payment value.
"""

corr_payment_value = corr.loc[(corr['payment_value'] > 0.3) | (corr['payment_value'] <= -0.3)]
corr_payment_value

"""We use the price, freight_value, and product_weight_g columns to create pairplots and visualize their relationship with the payment value. Since the dataset is too large to fit onto a single plot, we decide to randomly take 500 samples from the dataset. In this case, we have reasonable runtime for plotting while we can still visualize the general trend and relationships of the features. 

For better quality of the visualization and more readability, we remove outliers in columns. The plots are more zoomed in on the regions where the data points are more densely clusttered together. We also convert the unit of product weight from grams to kilograms so that the ticks become smaller and easier to understand.
"""

# Randomly sample 500 data points
full_df_sample = full_df.sample(500, random_state=seed)

# Convent product weight's unit from grams to kilograms
full_df_sample['product_weight_g'] = full_df_sample['product_weight_g'].apply(lambda x: x/1000)
full_df_sample = full_df_sample.rename(columns={'product_weight_g': 'product_weight_kg'})

# Remove outliers in payment_value
payment_value_q99 = full_df_sample['payment_value'].quantile(0.99)
full_df_sample = full_df_sample[full_df_sample['payment_value'] < payment_value_q99]

# Remove outliers in freight_value
freight_value_q99 = full_df_sample['freight_value'].quantile(0.99)
full_df_sample = full_df_sample[full_df_sample['freight_value'] < freight_value_q99]

# Remove outliers in product_weight
product_weight_q95 = full_df_sample['product_weight_kg'].quantile(0.95)
full_df_sample = full_df_sample[full_df_sample['product_weight_kg'] < product_weight_q95]

# Pairplot
g = sns.PairGrid(full_df_sample, hue='payment_type',
                 vars=['payment_value', 'price', 'freight_value', 'product_weight_kg'],
                 palette='Set2')
g.map_upper(sns.scatterplot)
g.map_diag(sns.histplot, hue=None, color='blue')
g.map_lower(sns.kdeplot, hue=None, cmap = sns.color_palette("mako", as_cmap=True))
g.add_legend()
g.fig.suptitle('Pairplot of Payment Value, Price, Freight Value, and Product Weight', y=1.01)
plt.show()

"""The visualization above demonstrates the pairwise relationships between the features. The diagnol plots are histograms of the features, each showing the counts of a feature at different values. The upper triangle above the diagnol are the plots of 2D scatter points with their x and y values being a pair of features. Note that the points are highlighted in different colors. Each color represents a type of payment. The types of payment are shown in the legend of the visualization. Below the diagnol are the KDE plots (Kernel Distribution Estimation Plots). These plots show the general trend of all samples and thus are not colored according to groups. The greenish/blueish contours indicate the center with the most density.

From the visualization, we obtain many useful insights. By looking at the scatter plots, we notice that orange points take up the majority of all points, indicating that most customers choose to use credit card as their payment method. This makes sense because our dataset is from a E-commerse platform just like Amazon and credit card is usually a safe and easy option.

Moreover, by looking at the histograms, we can get a sense of the distribution of each feature. For example, the product weight histogram clearly show that most items in this dataset are very light. Note that we removed outliers before creating this visualization. So we need to beware of that the lack of large product weights does not mean there is no heavy items.

Most importantly, we observe the pairwise relationships between features from the KDE plots shown below the diagnol. We can see that the positive correlation between payment value and price is the strongest compared to other pairs. From the shape of the contours near the center and how close together they are, we also get the sense of the density of these data samples, which can be generalized to the entire dataset.

### **4.2. Univariate Analysis**

#### **4.2.1. Customers**
"""

customer_columns = ['order_id', 'customer_id', 'customer_state']

plt.figure(figsize=(10, 6))
sns.countplot(x='customer_state', data=full_df[customer_columns], palette=("Blues_d"))
plt.title('Customer Distribution by State')
plt.xlabel('State')
plt.ylabel('Number of Customers')
plt.show()

"""The result indicates that the customer distribution by state is not even. It suggests that some states have a higher concentration of customers than others. This can be caused by a variety of factors. For example, the uneven distribution may be due to differences in the population size or density of different states. It could also be due to differences in the availability or accessibility of the product or service being offered. Additionally, uneven customer distribution by state could be the result of differences in marketing or sales efforts in different regions. It could also be due to the preferences or needs of customers in different states. This cause is hard to conclude, but we will do more analysis in the following sections.

#### **4.2.2. Order Status**

From the information in `order_status` we can know which step customers are fall into. Other than `delivered`, the final step in this order status system, there are steps such as `shipped`, `cancelled` etc. These order status information could be meaningful and become one of the factors influencing reviewers' scores.

For the status such as `canceled`, `unavailable` might potentially hurt the user experience and further decrease the ratings. Besides, when the status shows `shipped`, `processing` or any other pre-processing steps, there might be some errors occur that stopped the order process and prevented it moved to `delivered`.

Hence, we looked at the average review scores for these non-delivered orders below.
"""

shipped_review_mean = full_df[full_df['order_status'] == 'shipped']['review_score'].mean(axis=0)
canceled_review_mean = full_df[full_df['order_status'] == 'canceled']['review_score'].mean(axis=0)
invoiced_review_mean = full_df[full_df['order_status'] == 'invoiced']['review_score'].mean(axis=0)
processing_review_mean = full_df[full_df['order_status'] == 'processing']['review_score'].mean(axis=0)
unavailable_review_mean = full_df[full_df['order_status'] == 'unavailable']['review_score'].mean(axis=0)
approved_review_mean = full_df[full_df['order_status'] == 'approved']['review_score'].mean(axis=0)

print(f'The mean review score for "shipped" order is {shipped_review_mean}')
print(f'The mean review score for "canceled" order is {canceled_review_mean}')
print(f'The mean review score for "invoiced" order is {invoiced_review_mean}')
print(f'The mean review score for "processing" order is {processing_review_mean}')
print(f'The mean review score for "unavailable" order is {unavailable_review_mean}')
print(f'The mean review score for "approved" order is {approved_review_mean}')

"""All these ratings are equal or lower than 2 which is a pretty low rating in a 5.0 scale which indicates that the `order_status` can be factor that can decrease the rating score. However, the quantity of these special orders might be limited. We have plotted the occurances of these orders in the following plots."""

fig, axes = plt.subplots(1, 3, sharex=True, figsize=(20,5))
order_status_columns = ['order_id', 'order_status']

order_status_plot = sns.countplot(x='order_status', data=full_df[order_status_columns], ax=axes[0], palette=("Blues_d"))
order_status_plot.set(xlabel = 'Order Status', ylabel='Occurance', ylim=(0, None))
order_status_plot.tick_params(axis='x', rotation=90)

order_status_plot_med = sns.countplot(x='order_status', data=full_df[order_status_columns], ax=axes[1], palette=("Blues_d"))
order_status_plot_med.set(xlabel='Order Status', ylabel='Occurance', ylim=(0, 1200))
order_status_plot_med.tick_params(axis='x', rotation=90)

order_status_plot_smaller = sns.countplot(x='order_status', data=full_df[order_status_columns], ax=axes[2], palette=("Blues_d"))
order_status_plot_smaller.set(xlabel='Order Status', ylabel='Occurance', ylim=(0, 10))
order_status_plot_smaller.tick_params(axis='x', rotation=90)

fig.suptitle('Order Status Occurance')

"""### **4.3. Geospacial Analysis**
Use plotly or similar libraies to generate **interactive plots**

The shapefile of States and municiplcity of Brazil is imported for visualization on map. The choropleth is drawed with these geometries and there relative location on world map.
"""

states = geobr.read_state(year=2016)
municipalities = geobr.read_municipality(year=2016)
all_muni = geobr.read_municipality(code_muni="RJ", year=2016)
sp_muni = geobr.read_municipality(code_muni="SP", year=2016)

"""SÃ£o Paulo has the most 40k customers, larger than any of other states. Near by sta Rio de Janeiro and Minas Gerais also have around 10k users. The most popular cities are clustered together near the harbour. Ecommerce customers are almost uniformlly distributed across other states."""

#states["abbrev_state"] = states["abbrev_state"].str.lower()
customers_df_state=customers_df.groupby(by='customer_state')[['customer_id']].count().reset_index()
#customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()

# join geo with data
states_n = states.merge(customers_df_state, left_on="abbrev_state", right_on="customer_state")
states_n=states_n.rename(columns={'customer_id':'Numbers'})
fig = px.choropleth(states_n, geojson=states_n['geometry'], 
                    locations=states_n.index, color="Numbers",
                    height=500,
                    hover_name='abbrev_state',
                    labels = {'name_state' : 'Numbers'}, 
                    title = 'Numbers of customers across states',
                   color_continuous_scale="Viridis")
fig.update_geos(fitbounds="locations", visible=True)
fig.update_layout(
    title={'text':'Numbers of customers across states',
            'x':0.5,
            'xanchor': 'center'}
)
fig.show()

"""The pattern is very similar to customers, indicating the economics of state influence the commercial and consumption a lot. SÃ£o Paulo has the most 1.8k sellers, larger than any of other states. Near by sta Rio de Janeiro and Minas Gerais also have around 0.5k sellers. The most popular cities are clustered together near the harbour. Ecommerce customers are almost uniformlly distributed across other states."""

sellers_df_state=sellers_df.groupby(by='seller_state')[['seller_id']].count().reset_index()
#customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()

# join geo with data
states_seller = states.merge(sellers_df_state, left_on="abbrev_state", right_on="seller_state")
states_seller=states_seller.rename(columns={'seller_id':'Numbers'})
fig = px.choropleth(states_seller, geojson=states_seller['geometry'], 
                    locations=states_seller.index, color="Numbers",
                    height=500,
                    hover_name='abbrev_state',
                    labels = {'name_state' : 'Numbers'}, 
                    title = 'Numbers of customers across states',
                   color_continuous_scale="Viridis")
fig.update_geos(fitbounds="locations", visible=True)
fig.update_layout(
    title={'text':'Numbers of sellers across states',
            'x':0.5,
            'xanchor': 'center'}
)
fig.show()

"""Showing the number of customers across municipalities in the most popular state SÃ£o Paulo. Log is applied to retrieve better visualization"""

sp_muni["name_muni"] = sp_muni["name_muni"].str.lower()
customers_df_mul=customers_df.groupby(by='customer_city')[['customer_id']].count().reset_index()
# customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()
# join geo with data
sp_muni = sp_muni.merge(customers_df_mul,how='left', left_on="name_muni", right_on="customer_city")

sp_muni=sp_muni.rename(columns={'customer_id':'Numbers'})
sp_muni['Numbers']=sp_muni['Numbers'].apply(lambda x: np.log(x))
mean=sp_muni['Numbers'].mean()
tem=sp_muni['Numbers'].apply(lambda x: mean if pd.isna(x) else x)
sp_muni=sp_muni.rename(columns={'Numbers':'temp'})
sp_muni=pd.concat([sp_muni,tem], axis=1)

fig2 = px.choropleth(sp_muni, geojson=sp_muni['geometry'], 
                    locations=sp_muni.index, color="Numbers",
                    height=500,
                    hover_name='name_muni',
                    labels = {'name_state' : 'Numbers'}, 
                   )
fig2.update_geos(fitbounds="locations", visible=True)
fig2.update_layout(
    title={'text':'Numbers of customers across municipalities in SÃ£o Paulo',
            'x':0.5,
            'xanchor': 'center'}
)
fig2.show()

"""The total sales amount of different states shows a strong correlation with the total sellers in that state, which is predicatble."""

price_order_state = order_items_df.merge(sellers_df,left_on='seller_id',right_on='seller_id')
price_seller_state = price_order_state.groupby(by='seller_state')[['price']].sum().reset_index()
# customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()

# join geo with data
states_price_seller = states.merge(price_seller_state, left_on="abbrev_state", right_on="seller_state")
fig = px.choropleth(states_price_seller, geojson=states_price_seller['geometry'], 
                    locations=states_price_seller.index, color="price",
                    height=500,
                    hover_name='abbrev_state',
                    labels = {'name_state' : 'Numbers'}, 
                    title = 'Numbers of customers across states',
                   color_continuous_scale="Viridis")
fig.update_geos(fitbounds="locations", visible=True)
fig.update_layout(
    title={'text':'Total sales amount of sellers across states',
            'x':0.5,
            'xanchor': 'center'}
)
fig.show()

"""If we now explore the average order price of the states, we find out that the most popular states doesn't have the highest avg price, and the high price in Bahia and Pernambuco may indicate they are the origin of some expensive products."""

price_order_state=order_items_df.merge(sellers_df,left_on='seller_id',right_on='seller_id')
price_seller_state=price_order_state.groupby(by='seller_state')[['price']].mean().reset_index()
# customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()

# join geo with data
states_price_seller = states.merge(price_seller_state, left_on="abbrev_state", right_on="seller_state")
fig = px.choropleth(states_price_seller, geojson=states_price_seller['geometry'], 
                    locations=states_price_seller.index, color="price",
                    height=500,
                    hover_name='abbrev_state',
                    labels = {'name_state' : 'Numbers'}, 
                    title = 'Numbers of customers across states',
                   color_continuous_scale="Viridis")
fig.update_geos(fitbounds="locations", visible=True)
fig.update_layout(
    title={'text':'Average order price of sellers across states',
            'x':0.5,
            'xanchor': 'center'}
)
fig.show()

"""With further exploration, we found surprisingly that the most popular cities has the lowest average order price. Are people more frequently buying cheaper products in big cities? This may indicate an interesting purchase habit of people in different regions and this phenomena might be combined with our models in the ML part to give better business strategies."""

price_order_state=orders_df.merge(order_items_df,left_on='order_id',right_on='order_id')
price_order_state=price_order_state.merge(customers_df,left_on='customer_id',right_on='customer_id')
price_seller_state=price_order_state.groupby(by='customer_state')[['price']].mean().reset_index()
# customers_df_state['customer_state'] = customers_df_state['customer_state'].str.lower()

# join geo with data
states_price_seller = states.merge(price_seller_state, left_on="abbrev_state", right_on="customer_state")
fig = px.choropleth(states_price_seller, geojson=states_price_seller['geometry'], 
                    locations=states_price_seller.index, color="price",
                    height=500,
                    hover_name='abbrev_state',
                    labels = {'name_state' : 'Numbers'}, 
                    title = 'Numbers of customers across states',
                   color_continuous_scale="Viridis")
fig.update_geos(fitbounds="locations", visible=True)
fig.update_layout(
    title={'text':'Average purchase price of customer across states',
            'x':0.5,
            'xanchor': 'center'}
)
fig.show()

customers_df = pd.read_csv("./raw_data/olist_customers_dataset.csv")
geolocation_df = pd.read_csv("./raw_data/olist_geolocation_dataset.csv")
order_items_df = pd.read_csv("./raw_data/olist_order_items_dataset.csv")
order_payments_df = pd.read_csv("./raw_data/olist_order_payments_dataset.csv")
order_reviews_df = pd.read_csv("./raw_data/olist_order_reviews_dataset.csv")
orders_df = pd.read_csv("./raw_data/olist_orders_dataset.csv")
products_df = pd.read_csv("./raw_data/olist_products_dataset.csv")
sellers_df = pd.read_csv("./raw_data/olist_sellers_dataset.csv")
product_category_name_translation_df = pd.read_csv("./raw_data/product_category_name_translation.csv")

"""Map state abbreviation to state name for future join with shape file

### **4.4. Analysis on Text Data**

#### **4.4.1 NLP on review comment messages**

In the dataframe, we can observe lots of text data that might be valuable for anlaysis. We are going to split the contents of in the column into a list of words and will use the **nltk** library to facilitate text processing.

Cleaning `review_comment_message` column to replace empty or invaild values such as `NaN` for analysis.

We can first check the `info()` of the `review_comment_message` column:
"""

full_df[['review_comment_message']].info()

"""To be able to analyze the comment messages we get, we need to drop the NaN values first:"""

review_comment_message_dropna = full_df['review_comment_message'].dropna()
review_comment_message_dropna

"""With the cleaned data we can start using `nltk` library and evaluate the message content."""

nltk.download('punkt')

# Import `stopwords` library
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords_span = set(stopwords.words('spanish'))

"""Use the function `tokenize_content` to tokenize the review sentences for further analysis."""

# Tokenize a given string
def tokenize_content(content):
  words = [word.lower() for word in nltk.word_tokenize(content) if word.isalpha()]
  words = [word for word in words if word not in stopwords_span]
  return words

reviews = review_comment_message_dropna.tolist()
top_tokens_list = []

# Create top_tokens_list for all words in reviews
for review in reviews:
  token_words = tokenize_content(review)
  top_tokens_list.append(token_words)

top_tokens = [item for sublist in top_tokens_list for item in sublist]

"""Computing the `top_most_common` list for showing the words occuring most in the review comments."""

cnt = Counter()

# Count the occurances of words
for word in top_tokens:
  cnt[word] += 1

top_most_common = sorted(cnt.items(), key=lambda item: -item[1])[:100]

"""To visualize the outcomes, we used the wordcloud to show the words frequences."""

span_freq_dict = {}

# Creat top most common words dict with frequencies
for i in top_most_common:
  # Ignore the word if it is in stopwords in english
  span_freq_dict[i[0]] = i[1]

wc = WordCloud(background_color='white')
wc.generate_from_frequencies(span_freq_dict)

plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Since the messages are in spanish, in the followings steops, we translated the `top_most_common` to English and create a corresponding word cloud in English for better reading."""

translator = Translator()
top_most_common_en = list(map(lambda x: (translator.translate(x[0], dest="en").text, x[1]), top_most_common))

en_freq_dict = {}
top_word_count = 0

# Creat top most common words dict with frequencies
for i in top_most_common_en:
  # Ignore the word if it is in stopwords in english
  if (i[0] not in set(stopwords.words('english'))):
    en_freq_dict[i[0]] = i[1]
    top_word_count += i[1]

wc = WordCloud(background_color='white')
wc.generate_from_frequencies(en_freq_dict)

plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""In the word cloud above, we have printed out words based on the frequences it shows in the review comments' messages. The words that have larger sizes in the word cloud indicate that they are mentioned more in the messages which implies that they are properties most reviewers focused on. These information can provide us the directions of our research in which the words mentioned most in reviews can possibly be the factors that affect the ranking scores most.

We can easily find that the word 'product' has taken a big part in the reviews. We might consider the question about whether this means that most users reviewed about the quality of the product. However, although the word 'product' is ranked top 1 in the list, it can be used in commenting various aspects of the products and even other prespectives in the whole order process. To show the reviews with the word 'product' ('produto' in spanish), we took a sample of 500 reviews below to show how the reviews commenting around the product.



"""

sentences_with_produto_en = []
sample_reviews = sample(reviews, 500)
for review in sample_reviews:
  if 'produto' in review:
    sentences_with_produto_en.append(translator.translate(review, dest="en").text)
print(sentences_with_produto_en)

"""From the whole review sentences, we can observe that the reviews can probably commenting about quality of the product, its delivery process or missing/mistaken products etc. It is quite ambiguous to solely looking at the word 'product' and directly link it with the quality of the products. Hence, some other words might become more valuable. 

- **'received', 'has arrived', 'delivery', 'deadline' etc.**

   These words are ranked highly in the graph and all of them are delivery related words. In the following codes, the percentage of these words' frequencies in `top_most_common` words are calcuated which shows that in the `top_most_common` tuples list, these delivery related terms have taken a relatively large portion in all the common words. In order words, the delivery part in the whole ordering process can be a big factor affecting the ranking score.

- **'well', 'great', 'super', 'nao', 'right', 'Perfect' etc.**

   There is also a series of words that have a strong attitude associated with them, which can be potentially used for sentiment analysis.
"""

print(en_freq_dict)
delivery_words = ['I received', 'has arrived', 'delivery', 'deadline', 'I received', 'delivered', 'waiting', 'shipping', 'up until', 'packed']
delivery_words_freq_count = 0

for i in delivery_words:
  delivery_words_freq_count += en_freq_dict[i]

print(f'The delivery related words took {delivery_words_freq_count * 100/top_word_count: .1f}% of total top words\' occurances')

"""## **Section 5: Feature Engineering**

In this section, we perform feature engineering. Feature engineering is the process of transforming raw data into features that can be used to train a machine learning model. We hope to improve our modeling process through feature engineering.

First, we can improve the performance of our models by creating features that are more relevant and predictive. Moreover, we can reduce the complexity of your model by creating fewer, more meaningful features. We might consider techniques like PCA. This can make our models easier to interpret and can help prevent overfitting. Lastly, feature engineering can help us make the most of the data we have by extracting as much useful information as possible from it.

Note that most feature engineering we did are included in this section, but some others that relate to specific models are done right before the models. We think it makes more sense to leave those with their targeted model to mentain a logical flow for readers.

### 5.1. Solving imblance `review_score`

From the `review_score` prints and the plot below, we can notice that there is imblance among the scores where the score 5 has much higher counts compare with other scores. By handling that, we tried to combine some of the scores and made the `review_score` categorical which can decrease the imblance.

Create a new column called `review_score_factor` which can contain 3 values (0,1, 2) that respectively represent low (scoring 1,2), middle (scoring 3,4) and high (scoring 5) review score in the following manner:

`review_score_factor = 0, if review_score == 1 or review_score == 2`

`review_score_factor = 1, if review_score == 3 or review_score == 4`

`review_score_factor = 2, if review_score == 5`
"""

print("review_score == 5: %d" % len(full_df[full_df['review_score']==5]))
print("review_score == 4: %d" % len(full_df[full_df['review_score']==4]))
print("review_score == 3: %d" % len(full_df[full_df['review_score']==3]))
print("review_score == 2: %d" % len(full_df[full_df['review_score']==2]))
print("review_score == 1: %d" % len(full_df[full_df['review_score']==1]))
score_list = [len(full_df[full_df['review_score']==1]), 
              len(full_df[full_df['review_score']==2]),
              len(full_df[full_df['review_score']==3]),
              len(full_df[full_df['review_score']==4]),
              len(full_df[full_df['review_score']==5])]

plt.figure(figsize=(10,5))
chart = sns.barplot(x=[1,2,3,4,5], y=score_list)
chart.set(title="Review Score Count")
plt.show()

# Clone `full_df` into `feature_df` which will be used for modeling later
feature_df = full_df

# Stacking `review_score`
feature_df.loc[feature_df['review_score'] == 5, "review_score_factor"] = 2
feature_df.loc[feature_df['review_score'] <= 4, "review_score_factor"] = 1
feature_df.loc[feature_df['review_score'] <= 2, "review_score_factor"] = 0

feature_df['review_score_factor']

"""### **5.2. Changing `price` & `freight_value` to categorical**

As we explored before, we have three values, `payment_value`, `price`, and `freight_value`, that indicate a relatively high correlation. From the description in the dataset, we noticed that `payment_value` is the transaction value for the whole order which can be calculated from `price`, the item price, and `freight_value`, the item freight value. Thus, in the following modeling tasks, we would like to use the most comprehensive term, `payment_value`, as a dependent variable. Besides, since the rating is based on the whole order while `freight_value` and `price` are focusing on the individual items, the property `payment_value` will be more suitable for us in the analysis.

In the steps below, we will create two columns, `price_factor` and `freight_value_factor` respectively, for transforming `price` and `freight_value` to calegorical ones.

**For the `price_factor`:**

`price_factor = 0, if price < 50th percentile`

`price_factor = 1, if price >= 50th percentile`

**For the `freight_value_factor`:**

`freight_value_factor = 0, if freight_value < 50th percentile`

`freight_value_factor = 1, if freight_value >= 50th percentile`
"""

price_median = feature_df['price'].median();
feature_df.loc[feature_df['price'] >= price_median, "price_factor"] = 1
feature_df.loc[feature_df['price'] < price_median, "price_factor"] = 0

freight_value_median = feature_df['freight_value'].median();
feature_df.loc[feature_df['freight_value'] >= freight_value_median, "freight_value_factor"] = 1
feature_df.loc[feature_df['freight_value'] < freight_value_median, "freight_value_factor"] = 0

"""### **5.3. Package `delievered` or not**

In [section 4.2.2.](#scrollTo=e7GI0NmweEED), we have found the relationship between `order_status` and the review scores. In detail, a non-delivered order status might negatively affect the ratings. Instead of using all 7 status categories in the `order_status` columns, we will separate the data into `delivered` and `non-delivered` since we have a limited number of orders that don't have the `delivered` status and the differences among non-delivered categories, such as shipped, canceled, invoiced, couldn't provide much more information for predicting the ratings.
"""

feature_df.loc[feature_df['order_status'] == 'delivered', "if_delivered"] = 1
feature_df.loc[feature_df['order_status'] != 'delivered', "if_delivered"] = 0

"""### **5.4. Differences in estimated and actual delivery date**

When we ordered an item online, one of the factors we might care about is when we will receive the package. Whether the delievery is on-time or not can potential play an important role in ratings. Hence, in the codes below, we calculated `actual_est_delivery_diff` factor by using `order_delivered_customer_date` and `order_estimated_delivery_date` columns in the data.

First, we need to clean out some NaN data in the `order_delivered_customer_date`. Here, we make this value equal to the `order_estimated_delivery_date` in default if the row doesn't have information about `order_delivered_customer_date` so that `actual_est_delivery_diff = 0` wouldn't have too much affect on later prediction in this row.

On the other hand, from EDA before, we noticed that the lack of `order_delivered_customer_date` is mainly because its `order_status` isn't `delivered`. Thus, we can further use `if_delivered` we generated before for the predictions on these rows.
"""

# Check how many rows in non_delivered records don't have data in `order_delivered_customer_date`
non_delivered = feature_df[feature_df['if_delivered'] == 0]
nan_delivered_customer_count = non_delivered[non_delivered['order_delivered_customer_date'].isnull()]['order_id'].count()
print(f'There are {nan_delivered_customer_count} rows isnull in order_delivered_customer_date column for all non_delivered rows')

feature_df.loc[feature_df['order_delivered_customer_date'].isnull(), "order_delivered_customer_date"] = feature_df['order_estimated_delivery_date']

# Keep the difference in days to the column
feature_df['actual_est_delivery_diff'] = (feature_df['order_estimated_delivery_date'] - feature_df['order_delivered_customer_date']).dt.days

"""Therefore, when we see positive values in `actual_est_delivery_diff`, it means the customer has received the package earlier than estimated, which could be positively affecting the rating and vice versa.

### **5.5. How long it took to recieve the package**

Although we will be happy about getting a package earlier than expectation, the total duration we wait for the package can also have an impact on the rating. The longer customer needs to wait, the harder making a good rating score. As a result, in this section, we will create a new column, `delivery_duration` to store the duration customer waits for the order from purchase to order delivered.
"""

feature_df['delivery_duration'] = (feature_df['order_delivered_customer_date'] - feature_df['order_purchase_timestamp']).dt.days

"""### **5.6. Some NaN data**

Set the `product_photos_qty` and `product_description_length` to 0 by default if the original record is null here.

Drop the row if it doesn't have a weight. ðŸ‹ðŸ»
"""

feature_df.loc[feature_df['product_photos_qty'].isnull(), "product_photos_qty"] = 0
feature_df.loc[feature_df['product_description_length'].isnull(), "product_description_length"] = 0
feature_df = feature_df.dropna(subset=['product_weight_g'])

"""### **5.7. One Hot Encoding**

**Customer & Seller Location**
"""

# One-hot encode
d1 = pd.get_dummies(feature_df[['customer_state', 'seller_state']])
feature_df = pd.concat([feature_df, d1], axis=1)

"""**Product Category**

When listing the `product_category_name`, we found that some rows in `full_df` don't have any information about product name, product category, product description and product photos. These rows could possibly be some errors or testing data that are meaningless. Hence, before we one-hot encoding the product category, we cleaned the dataframe by removing the rows that has no product name.
"""

feature_df = feature_df.dropna(subset=['product_name_length'])

"""Now, for convenience, we merged the translated version for the product category name to the dataframe."""

feature_df = feature_df.merge(product_category_name_translation_df, on='product_category_name')

"""With the translated category name, we can now do some one-hot encoding on product category."""

d2 = pd.get_dummies(feature_df[['product_category_name_english']])
feature_df = pd.concat([feature_df, d2], axis=1)

df_summary(feature_df)

"""### **5.8. `review_comment_message` sentiment analysis**

`review_comment_message` listed the reviews customers have for an order which is a direct way to get their feelings about the order. The message itself will provide us valuable insights into the predictions of rating if we were able to identify the sentiment of the sentences.

`nltk` we used before for NLP has a library called `SentimentIntensityAnalyzer` includes a pre-trained sentiment analysis model. This powerful tool allowed us to evaluate and quantify each review's sentence and give a certain score to it. 

In this section, we will focus on the sentiment analysis on `review_comment_message` and added a new column, `comment_message_score`, to store all the sentiment scores.

You can use the code cell below to load the `feature_df` data with English transltions for reviews. Please remember to change the `ROOT` variable to a proper path to the csv file.

Either you can read the file locally,
"""

# Change `ROOT` to correct path
# ROOT = '/content/drive/MyDrive/Colab_Notebooks/CIS5450/final_project'
# FILE_NAME = 'features_with_review_translated.csv'
# SAVE_PATH = os.path.join(ROOT, FILE_NAME)

# try:
#  feature_df = pd.read_csv(SAVE_PATH)
# except:
#  print("No feature df with translation csv found, please add the csv file.")
#  print("The code to save feature_df to csv is in the 'Save to .csv' section")

"""Or you can read the csv file remotely"""

feature_df_csv_link = 'https://raw.githubusercontent.com/zywangdylan/Brazilian_Brazilian_E-Commerce_Product_Rating_Prediction/main/dataset/features_with_review_translated.csv'
full_df_csv_link = 'https://raw.githubusercontent.com/zywangdylan/Brazilian_Brazilian_E-Commerce_Product_Rating_Prediction/main/dataset/full_df_with_translated_review.csv'
feature_df = pd.read_csv(feature_df_csv_link)
full_df = pd.read_csv(full_df_csv_link)

"""The following codes are translating the `review_comment_message` and calculating the corresponding sentiment score for the reviews since the `SentimentIntensityAnalyzer` only allows to calculate sentiment score in English. 

The whole translating process will take around 1 hour for all the data. However, instead of waiting for an hour, you can directly use `features_with_review_translated.csv` file attached with all the translations and scores.
"""

# Uncomment the code if you need to translate all the reviews
# nltk.download('vader_lexicon')

# **DO NOT** run the cell if you already have `features_with_review_translated.csv` loaded
# Uncomment the codes if you need to translate all the reviews

# # libraries init
# sia = SentimentIntensityAnalyzer()
# translator = Translator()

# count = 0

# # translate each message and give a sentiment score
# def translateCommentMessage(message):
#   global count
#   translated = translator.translate(message, dest="en").text
#   score = sia.polarity_scores(translated)['compound']
#   print(f'{count}/{len(reviews)}: {score}')
#   count += 1
#   return translated, score

# # drop the rows that dont have a review
# with_comment_df = full_df.dropna(subset=['review_comment_message'])
# with_comment_df['review_comment_message_en'], with_comment_df['comment_message_score'] = zip(*with_comment_df['review_comment_message'].apply(translateCommentMessage))

# # Merge the with_comment_df to feature_df
# feature_df = feature_df.merge(with_comment_df[['order_id', 'comment_message_score', 'review_comment_message_en']], on='order_id', how='left').drop_duplicates()
# feature_df.loc[feature_df['comment_message_score'].isnull(), "comment_message_score"] = 0

"""### **Save to .csv**

Since translation will take a long time for such a big dataset, we save the results in a csv file. 

If you already have the translated file, you **don't need to run** the `to_csv` code below. ðŸ‘‡ðŸ»
"""

# You can change your saving path in the `ROOT` variable below
# feature_df.to_csv(SAVE_PATH, index=False)

# Save the translation
# SAVE_PATH = os.path.join(ROOT, 'full_df_with_translated_review.csv')
# with_comment_df.to_csv(SAVE_PATH, index=False)

"""## **Section 6: Models**

After doing all the hard work preparing data, we finally get to the exciting part of building models. In this section, we mainly focus on predicting ratings (review scores) which is a scalar score given by customers to indicate their level of satisfaction on orders. In predicting ratings, we have two setups: i) predict ratings not knowing the review text, and ii) predict ratings given the review text.

### **6.0 K-means Clustering**

From the EDA part, we already observed some different behaviors among customers in different region. It would be beneficitial if we could first explore our data with unsupervised machine learning to see if ceratin pattern is revealed. Either clustering result of k-means or grouping categories based on our EDA observation to split our data might improve our results
"""

drop_columns = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \
                'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_id', 'review_comment_title', 'review_score', \
                'review_creation_date', 'review_comment_message', 'review_answer_timestamp', 'payment_sequential', 'payment_type', 'customer_unique_id', \
                'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', \
                'freight_value', 'product_category_name', 'product_category_name_english', 'product_length_cm', 'product_height_cm', 'product_width_cm', \
                'seller_zip_code_prefix', 'seller_city', 'seller_state', 'review_comment_message_en', 'review_score_factor']


cluster_df = feature_df.drop(drop_columns, axis=1)
train, test = train_test_split(cluster_df, test_size=0.2,random_state=seed)
#standarlize first
scaler = StandardScaler()
train = scaler.fit_transform(train)
test = scaler.transform(test)

score=[]
cluster = list(range(5,18))
cluster = [int(l) for l in cluster]
for i in cluster:
  kmeans = KMeans(n_clusters=i, random_state=seed).fit(train)
  score.append(-kmeans.score(test))
plt.title("Kmeans within class distances")
plt.xlabel("cluster numbers")
plt.ylabel("sum of distances")
plt.grid(False)

plt.plot(cluster, score) 
plt.show()

"""The results indicate that a better cluster number would be higher than 15 which makes sense for such a big dataset, but if we keep building models on them, that would means we need to much more time (though training time for each subset is reduced) to train different model on them.

### **6.1 Regression Models**

#### **6.1.1. Linear Regression**

In this section, we perform a simple Linear Regression on the `feature_df` for the purpose of predicting the review score. After selecting the attributes as features and the `review_score` column as the label, we split the data into a training and testing sets with a size of 80% and 20%, respectively. The seed (random_state) is pre-defined in [section 1](#scrollTo=eTUIKV4eEf2a).
"""

rating = feature_df['review_score_factor']

drop_columns = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \
                'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_id', 'review_comment_title', 'review_score', \
                'review_creation_date', 'review_comment_message', 'review_answer_timestamp', 'payment_sequential', 'payment_type', 'customer_unique_id', \
                'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', \
                'freight_value', 'product_category_name', 'product_category_name_english', 'product_length_cm', 'product_height_cm', 'product_width_cm', \
                'seller_zip_code_prefix', 'seller_city', 'seller_state', 'review_comment_message_en', 'review_score_factor']

features = feature_df.drop(drop_columns, axis=1)

df_summary(features)

# getting the train and test data
x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(features, rating, test_size = 0.2, random_state = seed)

"""**Linear Regression (Unregularized)**

Starting from the simplest model, we use the `LinearRegression` class in scikit-learn to perform Linear Regression. Initialize a Linear regression model named `reg` with default parameters, fit the model to the training set, and then make predictions on the testing set.

Predictions are saved in an array named `y_pred_lr`, and R-squared score is reported in the variable `score`.
"""

# Initialize model with default parameters and fit it on the training set
reg = lm.LinearRegression().fit(x_train_lr, y_train_lr)

# Use the model to predict on the test set and save these predictions as `y_pred_lr`
y_pred_lr = reg.predict(x_test_lr)

# Find the R-squared score and store the value in `score`
score = reg.score(x_test_lr, y_test_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test_lr.values, y_pred_lr))

print(f'The R2 score in this linear regression model is {round(score, 5)}')
print(f'The RMSE in this linear regression model is {round(rmse_lr, 5)}, which is about {100*round(rmse_lr/np.mean(rating),2)}% of \
the mean rating score.')

"""The `score` value, R-squared value, is representing to how much variation of a dependent variable is explained by the independent variable(s) in a regression model, while the RMSE value is an absolute measure as it measures how much the predicted result deviates from the actual values, on average.

The linear regression above give us a R2 score around 0.2045 which indicates that this model is to some extend insufficient for explaining the results.

**Ridge Regression**

In this section, ridge regression, ð¿2  Regularized Linear Regression, is tried in the propose of reducing RMSE and promote the R2 score.
"""

lambdas = np.linspace(0.01, 100, 100)

ridgecv = lm.RidgeCV(alphas=lambdas, scoring='neg_mean_squared_error')
ridgecv.fit(x_train_lr, y_train_lr)
ridge_select = ridgecv.alpha_

# Initialize model with alpha = 10 and fit it on the training set
reg_ridge = lm.Ridge(alpha=ridge_select)
reg_ridge.fit(x_train_lr, y_train_lr)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred_lr = reg_ridge.predict(x_test_lr)

# Find the R-squared score and store the value in `ridge_score`
ridge_score = reg_ridge.score(x_test_lr, y_test_lr)
ridge_rmse_lr = np.sqrt(mean_squared_error(y_test_lr.values, y_pred_lr))

print(f'The R2 score in this ridge regression model is {ridge_score}')
print(f'The RMSE in this ridge regression model is {round(ridge_rmse_lr, 5)}, which is about {100*round(ridge_rmse_lr/np.mean(rating),2)}% of \
the mean rating score.')

"""However, from the R2 score and RMSE we calculated above, we found that there is limited improvement on the performance for applying Ridge Regression model since RMSE is 47% of the mean rating score which shows that the predicted values are far from the actual ones.

**Lasso Regression**

Since our data has more than one-hundred features, L1 regularization that eliminate some features might achieve better results.
"""

lambdas = np.linspace(0.00001, 0.001, 100)
lassocv = lm.LassoCV(alphas=lambdas)
lassocv.fit(x_train_lr, y_train_lr)
lassocv_select = lassocv.alpha_

# Initialize model with alpha = 10 and fit it on the training set
reg_lasso = lm.Lasso(alpha=lassocv_select)
reg_lasso.fit(x_train_lr, y_train_lr)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred_lr = reg_lasso.predict(x_test_lr)

# Find the R-squared score and store the value in `ridge_score`
lasso_score = reg_lasso.score(x_test_lr, y_test_lr)
lasso_rmse_lr = np.sqrt(mean_squared_error(y_test_lr.values, y_pred_lr))

print(f'The R2 score in this lasso regression model is {lasso_score}')
print(f'The RMSE in this lasso regression model is {round(lasso_rmse_lr, 5)}, which is about {100*round(lasso_rmse_lr/np.mean(rating),2)}% of \
the mean rating score.')

"""The Lasso has a very similar result to that of ridge.

#### **6.1.2. Logistic Regression (Classifier)**
"""

feature_df.loc[feature_df['review_score_factor'] == 2, "review_score_factor_boolean"] = 1.0
feature_df.loc[feature_df['review_score_factor'] < 2, "review_score_factor_boolean"] = 0.0

# store the binary classification target variable as "labels"
labels = feature_df["review_score_factor_boolean"]

drop_columns_log = drop_columns
drop_columns_log.append('review_score_factor_boolean')

features = feature_df.drop(drop_columns_log, axis=1)
combined_df = pd.concat([features, labels], axis=1)

x_train, x_test, y_train, y_test = train_test_split(features, labels, random_state = seed, test_size = 0.2)

# Initialize model with default parameters and fit it on the training set
start = time.time()
logreg = lm.LogisticRegression(penalty = 'l2', max_iter=1000)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = logreg.fit(x_train, y_train)
pred1 = logreg.predict(x_test)
end = time.time()
print(f"Time used for Logistic Reg: {end - start} sec\n")

# Find the accuracy and store the value in `log_acc`
log_acc = logreg.score(x_test, y_test)

print(f'The test accuracy is {round(log_acc, 5)}')

"""**Analysis on Logistic Regression model**"""

fig = sns.heatmap(confusion_matrix(y_test, pred1) / len(y_test), annot=True, fmt='.2%', cmap='Blues')
fig.axes.xaxis.set_ticks_position("top")

print(classification_report(y_test, pred1))

"""**Logistic Regression with PCA**

In order to get rid of any feature redundancies and removing the noise, PCA is used before building a Logistic Regression model. While reducing the dimensionality of a dataset, the overall interpretability of the data can be increased.

The cutoff line, 0.9, is selected for choosing proper number of components. In other words, we want our training features to collectively explain 90% of the variance.
"""

# Intermediate step to address scale-invariance
scaler = StandardScaler()
x_train_sc = scaler.fit_transform(x_train)
x_test_sc = scaler.transform(x_test)

# Instantiate and Fit PCA
pca = PCA(n_components=x_train_sc.shape[1])
x_train_pca = pca.fit_transform(x_train_sc)

# Use learned mean and SD to standardize testing set
x_test_pca = pca.transform(x_test_sc)

cum_expl_var_ratios_lr = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(cum_expl_var_ratios_lr)+1), cum_expl_var_ratios_lr)
plt.plot(np.arange(1, len(cum_expl_var_ratios_lr)+1), [0.90]*len(cum_expl_var_ratios_lr))
plt.ylabel("Explained Variance Ratio")
plt.xlabel("# of Components")
plt.title("# of Components vs. Explained Variance Ratio")
plt.grid(False)
plt.show()

for i in range(len(cum_expl_var_ratios_lr)):
  if cum_expl_var_ratios_lr[i] >= 0.90:
    print("We first surpass 90% explained variance with " + str(i + 1) + " components.")
    break

# The x train/test set with only the ideal 111 components
pca = PCA(n_components = 111)

x_train_pca = pca.fit_transform(x_train_sc)
x_test_pca = pca.transform(x_test_sc)

log_reg_pca = lm.LogisticRegression()
log_reg_pca.fit(x_train_pca, y_train)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred_pca`
y_pred_pca = log_reg_pca.predict(x_test_pca)

# Calculating test accuracy, RMSE and R2 score
score_pca_log = r2_score(y_test.values, y_pred_pca)
rmse_pca_log = np.sqrt(mean_squared_error(y_test.values, y_pred_pca))
test_accuracy = log_reg_pca.score(x_test_pca, y_test)

print(f'The test accuracy is {round(test_accuracy, 5)}')
print(f'The R2 score is {round(score_pca_log, 5)}.')
print(f'The RMSE score is {round(rmse_pca_log, 5)}.')

"""If we look at the test accuracy, we can see that there isn't an improvement to the model after doing PCA. Thus, there isn't collinearity between the variables or noise which might decrease the performance of the model. 

From the models above, we noticed that the relationship between the features and rating scores is probably non-linear, which explained the relatively low R2 score result from the previous models. However, the score from the logistic regression can be used as the baseline for further training.

#### **6.1.3. Linear classifiers with Stochastic Gradient Descent (SGD) training**

**Stochastic Gradient Descent** (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions. By applying SGD, the model will have a better efficiency. In this section, different loss functions are tried so as to explore the corresponding accuracy and time spent for them.
"""

start = time.time()
clf = make_pipeline(StandardScaler(), \
                    lm.SGDClassifier(loss='log', max_iter=1000, tol=1e-3))
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
end = time.time()
print(f"Time used for Logistic Reg: {end - start} sec\n")
print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))

losses = ["hinge", "log", "modified_huber", "perceptron", "squared_hinge"]
scores = []
times = []

for loss in losses:
    print(f'Computing for loss: {loss}')
    start = time.time()
    clf = make_pipeline(StandardScaler(), \
                        lm.SGDClassifier(loss=loss, max_iter=1000, tol=1e-3))
    clf.fit(x_train, y_train)
    score = clf.score(x_test, y_test)
    end = time.time()
    scores.append(score)
    times.append(end - start)

# Plot the loss functions' scores
plt.title("Effect of loss")
plt.xlabel("loss")
plt.ylabel("score")
plt.grid(False)
x = np.arange(len(losses))
plt.xticks(x, losses)
plt.plot(x, scores)

# Plot of training time for different losses
plt.title("Time used for training")
plt.xlabel("loss")
plt.ylabel("time used")
plt.grid(False)
x = np.arange(len(losses))
plt.xticks(x, losses)
plt.plot(x, times)

"""Hence, by applying SGD, the training processing time can be reduced while preserving accuracy.

### **6.2. Random Forest with Spark**

Next, we would try out a different machine learning model called Random forest to predict the rating of an order. 

It is a type of ensemble learning method that is used for classification and regression tasks in machine learning. It is called a "random forest" because it is a collection (forest) of decision trees, where each tree is trained on a random subset of the data. The final predictions of the random forest are made by averaging the predictions of all the individual decision trees. This ensemble approach helps improve the overall performance of the model and makes it more robust and less prone to overfitting. The use of random subsets of the data for training each decision tree also helps reduce the variance of the model, which is a common problem with decision trees. Overall, random forests are a powerful and widely-used machine learning method that can handle a variety of tasks and work well with both structured and unstructured data.The draw back of this powerful tool is that we would completely lost our interperbility of the model, where more precies prediction becomes our main goal

Another drawback of this algorithm is that it takes more time to go through with our more than 100 features. Therefore ,we put our eyes on the Apache Spark, which is the most popular distributed computing platform that we want to explore and utilize. We now use the same framework for distributed computation but only implement our single node since managing with AWS stack (which always expires after every few hours) takes time and brings in unstability. Generally, it is known for its fast and efficient processing of large-scale data sets, and it provides a wide range of tools and libraries for working with data, including support for SQL, machine learning, and stream processing. Spark is built on a cluster computing architecture, which allows it to distribute data and computation across multiple machines in a scalable and efficient manner. This makes it well-suited for working with very large datasets that cannot be processed on a single machine. Additionally, Spark includes a number of advanced features, such as in-memory computing and lazy evaluation, which further improve its performance and make it a popular choice for big data analytics.

First we need to set up the spark environment and create a spark session for our program.

#### 6.2.1. Set up spark environment
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt install libkrb5-dev
# !wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar xf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install findspark
# !pip install sparkmagic
# !pip install pyspark
# !pip install pyspark --user
# !pip install seaborn --user
# !pip install plotly --user
# !pip install imageio --user
# !pip install folium --user
# 
# from pyspark.sql import SparkSession
# from pyspark.sql.types import *
# import pyspark.sql.functions as F
# 
# spark = SparkSession.builder.appName('ml-hw4').getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

# Graph section
import networkx as nx
# SQLite RDBMS
import sqlite3

# NoSQL DB
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError, OperationFailure

os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

import pyspark
from pyspark.sql import SQLContext

try:
    if(spark == None):
        spark = SparkSession.builder.appName('Initial').getOrCreate()
        sqlContext=SQLContext(spark)
except NameError:
    spark = SparkSession.builder.appName('Initial').getOrCreate()
    sqlContext=SQLContext(spark)

"""#### 6.2.2. Tranform data into spark dataframe

With the environmnet set up, we would now transform our pandas dataframe to spark dataframe so it could be shraded and distributed on different nodes. Sometimes a schema specification is needed for this transform since spark dataframe has a different sets of datatypes than pandas, but because all our categorical feature is already turned into dummy variables, this step is omitted. For this specific prediction model, we would drop the review text out, since it is a feedback info that generated with the review score.Some unrelated original column is also dropped.
"""

drop_columns = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \
                'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_id', 'review_comment_title', 'review_score', \
                'review_creation_date', 'review_comment_message', 'review_answer_timestamp', 'payment_sequential', 'payment_type', 'customer_unique_id', \
                'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', \
                'freight_value', 'product_category_name', 'product_category_name_english', 'product_length_cm', 'product_height_cm', 'product_width_cm', \
                'seller_zip_code_prefix', 'seller_city', 'seller_state', 'review_comment_message_en']


rf_feature_df = feature_df.drop(drop_columns, axis=1)

# This optional schema is used to work on the original categorical pandas dataframe, where we could first transform to spark datafram and then 
#  deal with the dummy variables
mySchema = StructType([ StructField("payment_type", StringType(), True)\
                       ,StructField("payment_value", FloatType(), True),StructField("customer_city", StringType(), True)\
                       ,StructField("customer_state", StringType(), True),StructField("price", FloatType(), True)\
                       ,StructField("freight_value", FloatType(), True),StructField("product_category_name", StringType(), True)\
                       ,StructField("product_description_length", FloatType(), True),StructField("product_name_length", FloatType(), True)\
                       ,StructField("seller_city ", StringType(), True),StructField("seller_state", StringType(), True)\
                       ,StructField("delay", IntegerType(), True),StructField("review_score_factor", FloatType(), True)])
data_sdf = spark.createDataFrame(rf_feature_df)

"""To perform random forest in spark, we need to first convert the features to as what we want for our inputs, and split our data. Note that random forest is a scale-invariant method so that we doesn't need to normalize our data."""

from pyspark.ml.feature import StringIndexer, VectorAssembler
all_columns=data_sdf.columns
drop_columns=['review_score_factor'] # get rid of our prediction label and unrelated feature
columns_to_use =[i for i in all_columns if i not in drop_columns]
assembler = VectorAssembler(inputCols=columns_to_use, outputCol="features")
from pyspark.ml import Pipeline


pipe  = Pipeline(stages = [assembler])
modified_data_sdf = pipe.fit(data_sdf).transform(data_sdf)

# Do 80/20 train-test split with seed = random_seed and store them as "train_sdf" and "test_sdf"
train_sdf, test_sdf = modified_data_sdf.randomSplit([0.8, 0.2], seed = seed)

"""#### 6.2.3. Build up and train the model

We build random forest classifiers and fine tuning the hyperparameters (numTree,maxDepth) .The model that gives us the best result is shown below.
"""

# Import required libraries
from pyspark.ml.classification import RandomForestClassifier

# Instantiate  RF Model and call it `rf`. Then fit it to training data
rf = RandomForestClassifier(featuresCol="features",                            # Instantiate
                            labelCol="review_score_factor",
                            numTrees=40,
                            maxDepth=9,
                            seed=seed)
rf = rf.fit(train_sdf)

"""With the model built we than make predictions on our inputs and evluated the results.To further examine the prediction result of our model, we calculated the confusion matrix and displayed the result below."""

# Get predictions and save to "train_pred" and "test_pred" respectively
train_pred = rf.transform(train_sdf)
test_pred = rf.transform(test_sdf)

from pyspark.mllib.evaluation import MulticlassMetrics
preds_and_labels = train_pred.select(['prediction','review_score_factor']).withColumn('label', F.col('review_score_factor').cast(FloatType())).orderBy('prediction')

# Select only prediction and label columns
preds_and_labels = preds_and_labels.select(['prediction','label'])

metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))

rf_train_cm = metrics.confusionMatrix().toArray()
cm = rf_train_cm 
rf_train_accuracy = (cm[0,0] + cm[1,1])/(cm[0,0] + cm[1,1] + cm[1,0] + cm[0,1])
rf_train_accuracy 

# Evaluate predictions using accuracy for both train and test
# and call these `rf_train_accuracy` and `rf_test_accuracy` respectively
preds_and_labels = test_pred.select(['prediction','review_score_factor']).withColumn('label', F.col('review_score_factor').cast(FloatType())).orderBy('prediction')

# Select only prediction and label columns
preds_and_labels = preds_and_labels.select(['prediction','label'])

metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))

rf_test_cm = metrics.confusionMatrix().toArray()
cm = rf_test_cm
rf_test_accuracy = (cm[0,0] + cm[1,1])/(cm[0,0] + cm[1,1] + cm[1,0] + cm[0,1])

print("Test accuracy: {}".format(rf_test_accuracy))
print("Train accuracy: {}".format(rf_train_accuracy))

"""From the text data, we could see that the accuracy for predict 3 class rating (0-2, 3-4, 5) is 79%, which is higher than the regression model. Random forests combine multples smaller trees together to speed up the training while avoiding over-fit. We could see that the training accuracy is just slightly higher (2%) than the test accuracy, which agrees with the property of this algorithm. We would note that some categorical data that contains too much unique types would make the dataframe transforms into a huge wide sparse matrix with the unavoidable approach of dummy variables transformation. In particular, since we have thousands of different cities, there would be thousands of columns and would the cost much more to train the random forest. Random forest is better than regular linear regression in the sense that thousands of columns doesn't mean thousands of parameters since the depth of trees strictly restricted how many features are selected."""

sns.heatmap(metrics.confusionMatrix().toArray(), annot=True).set(title='Confusion metric for random forest classifer')

"""### **6.3. XGBoost**

Next, we would explore XGBoost, which tooks the advantage of combining multiple models as random forest. It implements the gradient boosting algorithm, which works by building an ensemble of weak models, such as decision trees in our case, and combining their predictions to produce a more accurate and robust final model. The key difference between XGBoost and random Forest is that XGBoost's learning of trees is not parallel. It increase the punishment on the mis-classification samples from last model and gradually adjust its weights. 

There are several advantages to using XGBoost for gradient boosting. One of the main advantages is that XGBoost is highly efficient and scalable. It has been designed to make efficient use of computational resources, which makes it well-suited for training large-scale models on large datasets. Additionally, XGBoost supports parallel computing, which can further improve its performance. Another advantage of XGBoost is that it includes a number of advanced features, such as regularization and support for various loss functions, which can help improve the performance and flexibility of the resulting models. Overall, XGBoost is a powerful and widely-used tool for gradient boosting that is known for its efficiency and effectiveness.

While we are exploring the result, we found that 3 class classfication would have relative low prediction and we then explore the binary classification rating (1-2, 3-5) which has more practical value.
"""

train, test = train_test_split(rf_feature_df, test_size=0.2,random_state=seed)

# Split feature and label with our dataframe from random forest
Y_train = train['review_score_factor']
X_train =  train.drop('review_score_factor', axis=1)
Y_test = test['review_score_factor']
X_test =  test.drop('review_score_factor', axis=1)

# Create a scaler object
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and test data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create an XGBoost classifier
model = xgb.XGBClassifier(max_depth=8, n_estimators=100)

# Train the model on the training data
model.fit(X_train_scaled, Y_train)

# Make predictions on the test set
predictions_te = model.predict(X_test_scaled)
predictions_tr = model.predict(X_train_scaled)

accuracy_tr = sum(Y_train == predictions_tr)/len(predictions_tr)
accuracy_te = sum(Y_test == predictions_te)/len(predictions_te)

print("Train accuracy: {}".format(accuracy_tr))
print("Test accuracy: {}".format(accuracy_te))

"""Now we try to further combine the rating and turn it into binary classification problem"""

# Split data into train and testï¼Œ inherited the one-hot encoding data from random forest
train, test = train_test_split(rf_feature_df, test_size=0.2, random_state=seed)

# Split feature and label with our dataframe from random forest
Y_train = train['review_score_factor']
X_train = train.drop('review_score_factor', axis=1)
Y_test = test['review_score_factor']
X_test = test.drop('review_score_factor', axis=1)
Y_train = Y_train.apply(lambda x: 1 if x>=1 else 0)
Y_test = Y_test.apply(lambda x:1 if x>=1 else 0)

# Create a scaler object
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and test data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create an XGBoost classifier
model = xgb.XGBClassifier(max_depth=8, n_estimators=100)

# Train the model on the training data
model.fit(X_train_scaled, Y_train)

# Make predictions on the test set
predictions_te = model.predict(X_test_scaled)
predictions_tr = model.predict(X_train_scaled)

accuracy_tr = sum(Y_train==predictions_tr)/len(predictions_tr)
accuracy_te = sum(Y_test==predictions_te)/len(predictions_te)

print("Train accuracy: {}".format(accuracy_tr))
print("Test accuracy: {}".format(accuracy_te))

"""After a training with similar tree depth and numbers which are our result of fine-tuning, it achieves 90% accuracy in our binary classification for rating. Though we used two different framework to compute XGBoost and random forest which makes the computation time hard to compare, but in general random forest trains faster under similar depth and model numbers and it would be even faster when we use parallel computing which is not achievable due to the incremental nature of XGBoost. However, XGBoost has a strong power to deal with inbalanced feature since it puts higher weights on the misclassification sample which solve the issue of minority samples being underepresent in the overall loss. In our case, since we already adjust the inbalance, the advantage of XGBoost might not be significantly better than Random Forest.

### **6.4. Feedforward Neural Network**

Now, we would to explore a more modern dl model: feedforward neural network,  a type of artificial neural network in which information flows through the network in only one direction, from the input layer to the output layer. It is roughly the simplest version of deep learning framework, but it still have the power to quite precies build non-linear model. In a feedforward neural network, the nodes in the input layer receive the input data, which is then processed by the hidden layers of the network using weights and biases. Finally, the output layer produces the desired output based on the processed input data. The weights and biases are typically adjusted during the training process to optimize the network's performance.

Some advantages of feedforward neural networks include their simplicity, flexibility, and ability to model complex nonlinear relationships. Because feedforward networks have a straightforward structure and only require data to flow in one direction, they are relatively easy to understand and implement. Additionally, feedforward networks can be easily customized by adding or removing hidden layers and nodes, which allows them to be applied to a wide range of problems. Finally, feedforward networks are capable of modeling complex nonlinear relationships between input and output data, which makes them well-suited for tasks such as classification and regression.
"""

torch.manual_seed(seed) 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""#### 6.4.1. Data process and Dataloader

Since our FNN doesn't includes cross-validation, we could not evaluate directly with our training loss since that could easily leads to overfitting. Here we manully split 10% part as validation set to fine tune our hyperparameters like epoch number.
"""

batch = 64

# Transform into binary classificationã€
train_label = Y_train.astype(int)
test_label = Y_test.astype(int)

# Create a tensor dataset from the dataframe
train_dataset = TensorDataset(torch.from_numpy(X_train_scaled), torch.from_numpy(train_label.to_numpy()))
test_dataset = TensorDataset(torch.from_numpy(X_test_scaled), torch.from_numpy(test_label.to_numpy()))

# Create validation set from train test
train_dataset, validation_dataset = train_test_split(train_dataset, test_size=0.1,random_state=seed)

train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True, num_workers=0)
validation_loader = DataLoader(validation_dataset, batch_size=batch, shuffle=True, num_workers=0)

"""#### 6.4.2. FNN architecture

The neural network we build here consist of a sigomid to scale the input data into 0-1, and four rounds of fully connected linear layer and relu activation layer. Though our inputs is wide ,we would still expand our features further to capture more infomation and then shrink back.
"""

class FNN(nn.Module):
    def __init__(self):
        super().__init__() 
        
        self.sigmoid = nn.Sigmoid()
        self.fc = nn.Linear(in_features=134, out_features=220)
        self.ReLU1 = nn.ReLU()
        self.fc2 = nn.Linear(in_features=220, out_features=300)
        self.ReLU = nn.ReLU()
        self.fc3 = nn.Linear(in_features=300, out_features=100)
        self.ReLU2 = nn.ReLU()
        self.fc4 = nn.Linear(in_features=100, out_features=3)
        # END TODO

    def forward(self, x):
        # TODO
        outputs = self.sigmoid(x)
        outputs = self.fc(x)
        outputs = self.ReLU1(outputs)
        outputs = self.fc2(outputs)
        outputs = self.ReLU(outputs)
        outputs = self.fc3(outputs)
        outputs = self.ReLU2(outputs)
        outputs = self.fc4(outputs)
        
        # END TODO
        return outputs

"""#### 6.4.3. Model training

The training process is rather standard. We use crossentropyloss to evaluate our loss since it is a classification problem.We choose a relatively small learning step since our data is wide and most features might not be strongly related.
"""

fnn = FNN().to(device)
criterion = nn.CrossEntropyLoss()
acc = nn.L1Loss()

optimizer = optim.Adam(fnn.parameters(), lr=1e-5) #lr - learning step
epoch = 30

loss_LIST_FNN = []
acc_LIST_FNN = []
val_acc_LIST_FNN = []
i = 0

# Train the Logistic Regression
for epoch in range(epoch):
  accuracy_r = 0.0
  running_loss = 0.0
  correct = 0
  total = 0
  total_v =0
  correct_v =0
  for inputs, labels in train_loader:
      labels = labels.type(torch.LongTensor) # Cast to Float
      inputs, labels = inputs.to(device), labels.to(device)
      inputs = inputs.to(torch.float32)
      labels = labels.to(torch.long)
      output = fnn(inputs)
      optimizer.zero_grad() # We need to reset the optimizer tensor gradient every mini-batch
      output = output.to(torch.float32)
   
      loss = criterion(output,labels) # this is the average loss for one mini-batch of inputs

      loss.backward() # Do a back propagation
      optimizer.step() # Update the weight using the gradients from back propagation by learning step
      running_loss += loss.item() #get the accumulated loss for each epoch
      
      _, predicted = torch.max(output.data, 1) # use max to get the prediction
      total += labels.size(0)
      correct += (predicted == labels).sum().item()
  
  for inputs, labels in validation_loader:
      labels = labels.type(torch.LongTensor) # Cast to Float
      inputs, labels = inputs.to(device), labels.to(device)
      inputs = inputs.to(torch.float32)
      labels = labels.to(torch.long)
      outputs = fnn(inputs)
        
      _, predicted_v = torch.max(outputs.data, 1) # use max to get the prediction
      total_v += labels.size(0)
      correct_v += (predicted_v == labels).sum()

  val_acc_FNN = 100 * correct_v / total_v
  
  accuracy = 100 * correct / total
        
  loss_LIST_FNN.append(running_loss / len(train_loader)) # get the avg loss for each epoch
  acc_LIST_FNN.append(accuracy)
  val_acc_LIST_FNN.append(val_acc_FNN)

  if i%3==0 :
    print("The loss for Epoch {} is: {}, Accuracy = {}".format(epoch, running_loss/len(train_loader), accuracy))

  i += 1

"""#### 6.4.4. Model Evaluation

Since training loss/ accuracy would increase continously, we introduce 10% validation set to evaluate our result and prevent our model from overfitting.
"""

epoch = 30
epoch = list(range(epoch))
accuracy = acc_LIST_FNN

fig = plt.figure(figsize=(10, 5))
loss = [1-x for x in loss_LIST_FNN]

# Creating the line plot
plt.plot(epoch, accuracy, label='train_accuracy')
plt.plot(epoch, val_acc_LIST_FNN, label='validation_accuracy')
plt.xlabel("epoch number")
plt.ylabel("accuracy")

plt.grid(False)
plt.legend()
plt.title("Training accuracy of different epoch of FNN")
plt.show()

"""The curve of validation accuracy has reached its platform and therefore our model has achieved best result in terms of tuning the epoch (30), and the result indicates that the FNN learns quite quick on our data and we could train with fewer epochs to save time."""

total = 0
correct = 0
test_acc_FNN = []

with torch.no_grad():
    for inputs, labels in test_loader:
        labels = labels.type(torch.LongTensor) # Cast to Float
        inputs, labels = inputs.to(device), labels.to(device)
        inputs=inputs.to(torch.float32)
        labels=labels.to(torch.long)
        outputs = fnn(inputs)
        
        _, predicted = torch.max(outputs.data, 1) # use max to get the prediction
        total += labels.size(0)
        correct += (predicted == labels).sum()

test_acc_FNN = 100 * correct / total
print('Test Accuracy: ' + str(test_acc_FNN))

"""The test accuracy is 88.8% which are good as our XGBoost model, but it still achieves a acceptable result.

Since the training time of our FNN is slower than others, now we would try rebuild a simpler and smaller FNN with dropouts to avoid overfitting on the result. A simpler neural network with fewer layers and few nodes per layers would speed up the training and save the time to fine-tune the hyperparamete
"""

class FNN_sp(nn.Module):
    def __init__(self):
        super().__init__() 
        
        #self.sigmoid = nn.Sigmoid()
        self.fc = nn.Linear(in_features=134, out_features=200)
        self.ReLU1 = nn.ReLU()
        self.fc3 = nn.Linear(in_features=200, out_features=80)
        self.ReLU2 = nn.ReLU()
        self.fc4 = nn.Linear(in_features=80, out_features=3)
        self.dropout = nn.Dropout(0.2)
      

    def forward(self, x):

        #outputs = self.sigmoid(x)
        outputs = self.fc(x)
        outputs = self.ReLU1(outputs)
        outputs = self.fc3(outputs)
        outputs = self.ReLU2(outputs)
        outputs = self.fc4(outputs)
        outputs = self.dropout(outputs)
        
        return outputs

fnn = FNN_sp().to(device)
criterion = nn.CrossEntropyLoss()
acc = nn.L1Loss()

optimizer = optim.Adam(fnn.parameters(), lr=2e-5) #lr - learning step
epoch = 20

loss_LIST_FNN = []
acc_LIST_FNN = []
val_acc_LIST_FNN = []
i = 0

# Train the Logistic Regression
for epoch in range(epoch):
  accuracy_r = 0.0
  running_loss = 0.0
  correct = 0
  total = 0
  total_v =0
  correct_v =0
  for inputs, labels in train_loader:
      labels = labels.type(torch.LongTensor) # Cast to Float
      inputs, labels = inputs.to(device), labels.to(device)
      inputs = inputs.to(torch.float32)
      labels = labels.to(torch.long)
      output = fnn(inputs)
      optimizer.zero_grad() # We need to reset the optimizer tensor gradient every mini-batch
      output = output.to(torch.float32)
   
      loss = criterion(output,labels) # this is the average loss for one mini-batch of inputs

      loss.backward() # Do a back propagation
      optimizer.step() # Update the weight using the gradients from back propagation by learning step
      running_loss += loss.item() #get the accumulated loss for each epoch
      
      _, predicted = torch.max(output.data, 1) # use max to get the prediction
      total += labels.size(0)
      correct += (predicted == labels).sum().item()
  
  for inputs, labels in validation_loader:
      labels = labels.type(torch.LongTensor) # Cast to Float
      inputs, labels = inputs.to(device), labels.to(device)
      inputs = inputs.to(torch.float32)
      labels = labels.to(torch.long)
      outputs = fnn(inputs)
        
      _, predicted_v = torch.max(outputs.data, 1) # use max to get the prediction
      total_v += labels.size(0)
      correct_v += (predicted_v == labels).sum()

  val_acc_FNN = 100 * correct_v / total_v
  
  accuracy = 100 * correct / total
        
  loss_LIST_FNN.append(running_loss / len(train_loader)) # get the avg loss for each epoch
  acc_LIST_FNN.append(accuracy)
  val_acc_LIST_FNN.append(val_acc_FNN)

  if i%3==0 :
    print("The loss for Epoch {} is: {}, Accuracy = {}".format(epoch, running_loss/len(train_loader), accuracy))

  i += 1

epoch = 20
epoch = list(range(epoch))
accuracy = acc_LIST_FNN

fig = plt.figure(figsize=(10, 5))
loss = [1-x for x in loss_LIST_FNN]

# Creating the bar plot
plt.plot(epoch, accuracy)
 
plt.xlabel("epoch number")
plt.ylabel("accuracy")
plt.xticks(epoch)
plt.grid(False)
plt.plot(epoch, val_acc_LIST_FNN, label='validation_accuracy')
plt.title("Training accuracy of different epoch of simpler FNN")
plt.show()

total = 0
correct = 0
test_acc_FNN = []

with torch.no_grad():
    for inputs, labels in test_loader:
        labels = labels.type(torch.LongTensor) # Cast to Float
        inputs, labels = inputs.to(device), labels.to(device)
        inputs = inputs.to(torch.float32)
        labels = labels.to(torch.long)
        outputs = fnn(inputs)
        
        _, predicted = torch.max(outputs.data, 1) # use max to get the prediction
        total += labels.size(0)
        correct += (predicted == labels).sum()

test_acc_FNN = 100 * correct / total
print('Test Accuracy: ' + str(test_acc_FNN))

"""Though we observed a quicker convergence of train_prediction as expected, but the accuracy also decreased to 85%. There are also larger fluctuation among the accuracy which might be the result of drop out of nodes in hidden layer. These indicates that our more complex FNN model are needed to capture the more info.

### **6.5. LSTM Neural Network**

In this section, we try to use the long short-term memory (LSTM) neural network to predict review scores based on the content of reviews. A LSTM Neural Network is a type of recurrent neural network (RNN) that is capable of learning long-term dependencies in data. Unlike traditional RNNs, which use a simple update rule to propagate information through time, LSTMs use a more complex mechanism called a memory cell to store and propagate information. This allows LSTMs to effectively remember past information and use it to make predictions.

In general, we perform the following steps to use an LSTM to predict review scores based on reviews. We first train a model on a dataset of reviews and their corresponding ratings. This would involve preprocessing the text data to convert it into a numerical representation that can be fed into the LSTM. This can be done using techniques such as tokenization, which breaks the text into individual words or phrases, and embedding, which maps each word or phrase to a dense vector in a high-dimensional space. Once the data has been preprocessed and converted into a numerical representation, we train an LSTM model on the data.

We leverage the spaCy library here. spaCy is a popular open-source library for NLP in Python. It is designed to help developers build applications that can process and understand large amounts of text efficiently and accurately. One of the key features of spaCy is its ability to perform tokenization, which is the process of segmenting text into individual words or phrases. It also includes tools for part-of-speech tagging, named entity recognition, and more.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install spacy
# ! python -m spacy download pt_core_news_sm
# import torch.nn.functional as F
# import spacy

"""**Set GPU**"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

"""#### **6.5.1. Cleaning up and tokenize the reviews**

For building this LSTM model, we focus on only features related to reviews. In this case, we have `review_comment_title` and `review_comment_message`. In addition, we will the the `review_score` as our labels.

We use the `fillna()` method to replace any missing values in columns with an empty string. By replacing the missing values with empty strings, we ensure that the data in the `review_comment_title` and `review_comment_message` columns can be processed by the rest of the program without errors. It also allows the program to continue running even if some of the data is missing.

Then, we concatenate strings in the `review_comment_title` and `review_comment_message` columns so that each review title and review content becomes one single review. Both the title and the text could be informational. By doing this step, we are not leaving out parts of the the information.

We also calculated the `review_length` and saved it as a column for later use.
"""

reviews_feature_df = feature_df[['review_comment_title', 'review_comment_message', 'review_score']].copy()

# Fill null values with empty string
reviews_feature_df['review_comment_title'] = reviews_feature_df['review_comment_title'].fillna('')
reviews_feature_df['review_comment_message'] = reviews_feature_df['review_comment_message'].fillna('')

# Concatenate review title and review text
reviews_feature_df['review'] = reviews_feature_df['review_comment_title'] + ' ' + reviews_feature_df['review_comment_message']

# Only keep the concatenated column and the score
reviews_feature_df = reviews_feature_df[['review', 'review_score']]

# Calculate length of review
reviews_feature_df.loc[:, 'review_length'] = reviews_feature_df.loc[:, 'review'].apply(lambda x: len(x.split()))

# Make the review score start from 0
# Now scores are 0 ~ 4
start_from_zero = {1:0, 2:1, 3:2, 4:3, 5:4}
reviews_feature_df['review_score'] = reviews_feature_df['review_score'].apply(lambda x: start_from_zero[x])

print(f"The average review length is {reviews_feature_df['review_length'].mean()}")

"""Here we use the `spacy.load()` function to load a pre-trained Portuguese language model called "pt_core_news_sm" from the spaCy library. This model can be used to perform various NLP tasks on Portuguese text. We use it to tokenize the reviews in the DataFrame.

In the `tokenize` function, we first do some cleaning. We use the `re.sub()` function to perform a text substitution on the given text string. The regular expression matches any character that is not in the range of ASCII characters from 0 to 127 (inclusive). It replaces any such non-ASCII characters with a space character, effectively removing them from the text. Then, we remove punctuation and digits from the text. The `re.escape()` function is used to escape any special characters in the punctuation string so that they are treated as literal characters in the regular expression. Once the regular expression pattern is created, it is compiled into a regex object using the `re.compile()` function. This object is then used with the `re.sub()` function to replace all occurrences of the matched characters in the text with a space character. By doing these steps, we remove the punctuation and digits from the text, leaving only the remaining words.
"""

tok = spacy.load('pt_core_news_sm')
def tokenize(text):
    text = re.sub(r"[^\x00-\x7F]+", " ", str(text))
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    no_punct = regex.sub(" ", text.lower())
    return [token.text for token in tok.tokenizer(no_punct)]

"""Create a `Counter()` to get the number of occurences of each word. Then remove words that are infrequent. We remove infrequent words because infrequent words are often less informative and less predictive of the task at hand than more frequent words. By removing these words, we can reduce the size of the vocabulary and make the model more efficient and effective. Additionally, removing infrequent words can help to prevent overfitting, which occurs when a model is too complex and fits the training data too closely. Overfitting can cause a model to perform poorly on new, unseen data, so removing infrequent words can help to prevent this problem."""

counts = Counter()
for _, row in reviews_feature_df.iterrows():
    counts.update(tokenize(row['review']))

# Remove word with count less than 2
print("Number of words before removal:",len(counts.keys()))
for word in list(counts):
    if counts[word] < 2:
        del counts[word]
print("Number of words after removal:",len(counts.keys()))

"""#### **6.5.2. Create encodings**
We create a `vocab_to_index` mapping before training an LSTM model. Our LSTM model operates on numerical data, so we need to convert the words in the text data into numerical representations that the model can understand. The vocab-to-index mapping is basically a dictionary that maps each unique word in the vocabulary to a unique integer index. This allows us to represent each word in the text data as a unique integer, which can be fed into the LSTM model as input.
"""

vocab_to_index = {"":0, "UNK":1}
words = ["", "UNK"]
for word in counts:
    vocab_to_index[word] = len(words)
    words.append(word)

"""Using the `vocab_to_index` mapping, we convert reviews into encodings."""

def create_sentence_encoding(sentence, vocab_to_index, max_length=30):
    words = tokenize(sentence)
    encoded = np.zeros(max_length, dtype=int)
    # The full length of sentence encoding
    encoded_full = np.array([vocab_to_index.get(word, vocab_to_index["UNK"]) for word in words])
    length = min(max_length, len(encoded_full))
    # The encoding is capped at max_length
    encoded[:length] = encoded_full[:length]
    return encoded, length

reviews_feature_df['encoded'] = reviews_feature_df['review'].apply(lambda x: np.array(create_sentence_encoding(x, vocab_to_index), dtype=object))
reviews_feature_df.head()

"""As shown above, now we have a DataFrame containing a column with the encoded reviews.

We print out the count of each class. Notice that the classes are imbalanced. There are more many more 4 than other classes.
"""

Counter(reviews_feature_df['review_score'])

"""#### **6.5.3. Prepare Pytorch Dataset**

We first do a train/val split. We have 80% of data to be used for training and 20% left out for validation. (Here we perform validation and report validation statistics every epoch.)
"""

X = list(reviews_feature_df['encoded'])
y = list(reviews_feature_df['review_score'])

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

"""We override the Dataset class in PyTorch: we create a new class that inherits from the Dataset class and implements its required methods. We implement the `__len__()` and `__getitem__()` methods so that it understands the structure of the encoded sentence tuple and returns data accordingly."""

class ReviewsDataset(Dataset):
    def __init__(self, X, Y):
        self.X = X
        self.y = Y
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]

train_ds = ReviewsDataset(X_train, y_train)
valid_ds = ReviewsDataset(X_val, y_val)

"""We set a batch size and feed the training and validation data into PyTorch's `DataLoader`."""

BATCH_SIZE = 1000
train_data_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_data_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)

"""#### **6.5.4. Training**
Here we define the training loop, which is a sequence of steps that are executed in each epoch of the training process.

It mainly consists of the following steps:

1. Load the training data: The first step is to load the training data throught the data loader. This will return a batch of data in each iteration of the loop.

2. Clear the gradients: Before starting the forward pass, the gradients of the model's parameters need to be cleared. This is because PyTorch accumulates gradients by default, so we need to reset them at the start of each iteration.

3. Forward pass: Next, the input data is passed through the model to compute the predicted output.

4. Compute the loss: The predicted output is then compared to the true output using a loss function, which computes a scalar value that measures the difference between the two. This loss value is used to update the model's parameters and improve its performance. In our model, we use the `cross_entropy` loss function.

5. Backward pass: After the loss has been computed, the gradients of the model's parameters are computed by calling the `backward()` method on loss.

6. Update the parameters: Finally, the model's parameters are updated using the `Adam` optimizer. This updates the parameters based on the computed gradients and the learning rate.

7. Evaluate the performance of the model by calculating the loss, accuracy, and RMSE on the validation data. We also keep track of these data through the training process.

These steps repeat for each epoch of the training loop.
"""

def train_model(model, train_data_loader, val_data_loader, epochs=25, lr=0.01):
    parameters = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = torch.optim.Adam(parameters, lr=lr)

    train_loss_hist = []
    val_loss_hist = []
    val_acc_hist = []
    val_rmse_hist = []

    for i in range(epochs):
        model.train()
        sum_loss = 0.0
        count = 0
        for x, y, length in train_data_loader:
            x, y = x.long().to(device), y.long().to(device)
            y_pred = model(x, length)
            optimizer.zero_grad()
            loss = F.cross_entropy(y_pred, y)
            loss.backward()
            optimizer.step()
            sum_loss += loss.item()*y.shape[0]
            count += y.shape[0]

        val_loss, val_acc, val_rmse = validate(model, val_data_loader)
        train_loss_hist.append(sum_loss/count)
        val_loss_hist.append(val_loss)
        val_acc_hist.append(val_acc.item())
        val_rmse_hist.append(val_rmse)
        print("Epoch %2d: train loss %.3f ||| val loss %.3f | val accuracy %.3f | val rmse %.3f" % (i+1, sum_loss/count, val_loss, val_acc, val_rmse))
    return train_loss_hist, val_loss_hist, val_acc_hist, val_rmse_hist

def validate(model, val_data_loader):
    model.eval()
    correct = 0
    count = 0
    sum_loss = 0.0
    sum_rmse = 0.0
    for x, y, length in val_data_loader:
        x, y = x.long().to(device), y.long().to(device)
        y_hat = model(x, length)
        loss = F.cross_entropy(y_hat, y)
        pred = torch.max(y_hat, 1)[1]
        correct += (pred == y).float().sum()
        count += y.shape[0]
        sum_loss += loss.item()*y.shape[0]
        sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), y.unsqueeze(-1).cpu()))*y.shape[0]
    return sum_loss/count, correct/count, sum_rmse/count

"""We define the model architecture of our LSTM neural network. In the forward pass, the model takes a batch of text data and the corresponding sequence lengths and returns the predicted output. 

The forward pass first uses the `nn.Embedding` layer to convert the input data into word embeddings, then applies the `nn.LSTM` layer to process the sequence of word embeddings. The LSTM layer processes the input word embeddings by iterating over them in sequence and using the previous hidden state and the current input to update its internal state and produce an output. This allows the LSTM layer to capture contextual information and dependencies between the words in the input sequence, which is one of the key strengths of LSTM networks. After that, the hidden state of the LSTM layer is then passed through several `nn.Linear` layers with ReLU activation to produce the final output.

We also include dropout layers to avoid overfitting.
"""

class LSTM_NN(torch.nn.Module) :
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim) :
        super().__init__()
        self.dropout = nn.Dropout(0.3)
        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc1 = nn.Linear(hidden_dim, hidden_dim*40)
        self.fc2 = nn.Linear(hidden_dim*40, hidden_dim*20)
        self.fc3 = nn.Linear(hidden_dim*20, output_dim)
        self.relu = nn.ReLU()
        
    def forward(self, x, s):
        x = self.embeddings(x)
        x = self.dropout(x)
        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)
        out_pack, (ht, ct) = self.lstm(x_pack)
        x = self.fc1(ht[-1])
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.relu(x)
        out = self.fc3(x)
        return out

"""Create model."""

vocab_size = len(words)
model = LSTM_NN(vocab_size, 
                embedding_dim=30, 
                hidden_dim=20, 
                output_dim=5).to(device)

"""Run training."""

# Hyperparameters
EPOCHS = 50
LEARNING_RATE = 0.001

# Run training
history = train_model(model, train_data_loader, val_data_loader, epochs=EPOCHS, lr=LEARNING_RATE)

"""#### **6.5.5. Performance Evaluation**

First we plot **train loss v.s. validation loss**. The plot shows that the optimization correctly minimizes the loss.
"""

epochs = np.arange(1, EPOCHS+1)

def plot_loss(epochs, history):
    plt.plot(epochs, history[0], label='train loss')
    plt.plot(epochs, history[1], label='val loss')
    plt.title('Train loss v.s. val loss history')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(False)
    plt.show()

plot_loss(epochs, history)

"""Furthermore, we plot **Validation accuracy**. It indicates that our model easily beats the majority baseline. However, we believe it's more meaningful to look at the RMSE plot below rather than using accuracy for evaluation. The reasons are explained in details later."""

label_counts = Counter(reviews_feature_df['review_score'])
majority_baseline = max(label_counts.values())/sum(label_counts.values())

def plot_accu(epochs, history, majority_baseline):
    plt.plot(epochs, history[2], label='LSTM-NN')
    plt.axhline(y=majority_baseline, color='black', label='majority baseline')
    plt.title('Validation accuracy history')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(False)
    plt.show()

plot_accu(epochs, history, majority_baseline)

"""Lastly, we plot **Validation RMSE**. RMSE (root mean squared error) is a commonly used metric for evaluating the performance of a model. It is the square root of the mean squared error, which is the average squared difference between the predicted values and the true values.

Looking at RMSE is actually more meaningful than looking at the accuracy. Our task which uses reviews to predict ratings is difficult. People have different standards for scores of 1 through 5. For some people, a useable product deserves a 5 while for some others, only a perfect product can get a 5.

In tasks where the target variable has a high level of subjectivity, such as predicting ratings based on reviews, it can be more meaningful to evaluate the performance of a model using a metric that measures the magnitude of the error rather than just the accuracy. This is because the level of subjectivity can make it difficult to determine whether a predicted value is "correct" or not, as different people may have different standards for what constitutes a "correct" rating.

In these situations, using RMSE can be more informative and useful than using accuracy. RMSE measures the magnitude of the error in the predicted values, and it is in the same units as the original data. This means that we can easily interpret the RMSE value and understand how far off our predictions are on average.

Looking at the RMSE plot below, we observe that the RMSE comes close to almost 1.1 which means in most of the times, our preditions are only off by approximately 1. Such performance is good enough to provide a reasonable estimate based on the review text. In cases where human readers examine the reviews, human also have difficulties to predict the correct rating. We think our model's performance is at least similar to the performance of human (not to mention our model is capable of batch processing).


"""

def plot_rmse(epochs, history):
    plt.plot(epochs, history[3])
    plt.title('Validation RMSE history')
    plt.xlabel('Epoch')
    plt.ylabel('RMSE')
    plt.grid(False)
    plt.show()

plot_rmse(epochs, history)

"""#### **6.5.6. Train the same model with binary labels**

After realizing how difficult it is to classify ratings of 1 through 5, we want to try making our task easier. We can map 5 labels to binary labels (only 0 and 1). We think this will reduce the difficulties in making predictions. The results will still be meaningful because now we basically turn the problem into predicting whether the customer is satisfied or not satisfied based on his/her review. 

The training process is generally the same with some trivial tweaks, so we abbreviate the explanations for training. Feel free to skip to the results.

We map 5 labels to 2 labels.
"""

binary_labels = {0:0, 1:0, 2:0, 3:1, 4:1}
reviews_feature_df_binary = reviews_feature_df.copy()
reviews_feature_df_binary['review_score_binary'] = reviews_feature_df_binary['review_score'].apply(lambda x: binary_labels[x])
reviews_feature_df_binary

X = list(reviews_feature_df_binary['encoded'])
y = list(reviews_feature_df_binary['review_score_binary'])

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

train_ds_binary = ReviewsDataset(X_train, y_train)
valid_ds_binary = ReviewsDataset(X_val, y_val)

batch_size = 1000
train_data_loader = DataLoader(train_ds_binary, batch_size=batch_size, shuffle=True)
val_data_loader = DataLoader(valid_ds_binary, batch_size=batch_size)

vocab_size = len(words)
model_binary = LSTM_NN(vocab_size, 
                       embedding_dim=30, 
                       hidden_dim=20, 
                       output_dim=2).to(device)

# Hyperparameters
EPOCHS = 50
LEARNING_RATE = 0.0005

# Run training
history_binary = train_model(model_binary, train_data_loader, val_data_loader, epochs=EPOCHS, lr=LEARNING_RATE)

"""**Results of our model for binary labels:**

After simplifing the problem with binary labels, our model is able to achieve a much higher accuracy (close to 90%). 

This result makes sense because predict whether a customer is satisfied is much easier than predict the given rating on a scale of 5.

In practice, we think it is very worthy to take tradeoff of losing the specific rating for a higher accuracy. Knowing the specfic rating of 1 through 5 is definitely helpful, but it is more important to know whether customers are satisfied. Sellers can adjust their products and strategy accordingly based on customer satisfaction. Our model for binary labels can produce a high accuracy. If sellers use this model, they can have higher confidence in decision making based on the prediction results.
"""

epochs = np.arange(1, EPOCHS+1)
label_counts = Counter(reviews_feature_df_binary['review_score_binary'])
majority_baseline_binary = max(label_counts.values())/sum(label_counts.values())
plot_loss(epochs, history_binary)
plot_accu(epochs, history_binary, majority_baseline_binary)
plot_rmse(epochs, history_binary)

"""## **Section 7: Conclusion**

In this project, we have studied the **[Brazillian E-commerce Public Dataset by Olist dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)** from Kaggle to find solutions for predicting the rating score of the product so that we can understand consumer behavior, identify trends and patterns, develop feasible prediction models and further improve the customer experience via analysis on the whole shopping service.
 
This E-commerce dataset is a typical example of a large dataset in the aspect of its number of records, dimensions, and variety of data types, which can provide us with multiple entry points for predicting the rating scores. Through some steps of data wrangling and cleaning, we can output a corrected and more comprehensive version of the data for exploration and analysis later on while some basic summaries of the data are generated so that we can have a closer look at the structure of the dataset.
 
<br/>
 
**Exploratory Data Analysis (EDA)**
 
Before building models, Exploratory Data Analysis (EDA) is established to help us identify any anomalies that may affect modeling and indicate some small patterns among features before generating a general analysis or modeling on the whole dataset.
 
Firstly, in order to investigate the relationships between different variables, we created a correlation heatmap on all the numeric columns. For the most correlated variables, price, freight_value, and product_weight_g, we have visualized and evaluated their relationship and distribution by applying pairwise plots and Kernel Distribution Estimation plots. We found that the payment value and price have the strongest correlation which should be optimized during feature engineering.
 
On the other hand, some text-type columns, such as customer locations and order status, are explored individually through univariate analysis. The customer distribution by sale plot shows the state SP has the largest number of customers which might result from the uneven distribution of the population and possibly having more resources in marketing, logistics, services and etc. which can lead to positive review scores. Besides, we were able to visualize the distribution of order status that mostly holds the value, delivered, and only has several other statuses that might be caused by an error occurring during the delivery of products.
 
The geography-based section of our EDA has dug in geo-related information indicated in the dataset. We have visualized the differences in the numbers of customers and sellers across states in Brazil and cities in the most popular state, SÃ£o Paulo. Also, more price-related features are plotted on the map, such as total sales amount, average order prices, and average purchase prices. These plots directly point out people's different purchase habits across various regions.
 
Another important factor we considered is the review comment messages. We have constructed an NLP analysis to specify the top common words that appear in all the review comments with the help of a word cloud. The common words in the reviews suggested the perspectives customers caring most when reviewing a product or order.
 
<br/>
 
**Models**
 
To make the dataset more suitable for training, we have implemented feature engineering to balance the review_score data, reduce the collinearity among variables by changing some columns to numerical, and apply one hot encoding for some categories such as customer & seller locations and order status. After that, we trained several different models on the same engineered data frame to see how different models perform in prediction.
 
- *K-means clustering*

 Since our data includes orders across the whole Brazil country and almost every category, it would be bette if we could first split the data and build different models on each.The clustering score reports that over 15 groups would be be best to split our data. However, after the such a spliting, our data records in each group drops a lot, inbalance class are brought in again and are different in each groups. Besides, we need to build and reports 15 times more models which drasticlly increased our workload.

- *Regression Models*
 
 From the simplest, regression models are first tried for setting base reference lines in later training. Overall, the performance of regression models is underwhelming. The linear regression model only reaches around 0.20 in the R2 score and has a relatively high RMSE which proves that the predictions are far from actual. Ridge regulation is implemented as well, however, it doesn't help much. Compare to the models before, logistic regression has better accuracy. We used the binary labels for classification using logistic regression. By plotting the confusion matrix, although it has a test accuracy of around 66%, the false positive value is relatively high and used around 35.5 seconds to train and PCA doesn't have much in promoting accuracy. To train with better efficiency, we also tested on Stochastic Gradient Descent for logistic regressions along with some other loss functions. Although this technique can not increase accuracy, the training time is reduced to around 4 seconds.
 
- *Random Forest*
 
 Next, we used the random forest to make the model robust and less prone to overfitting. The feature of restricted features involved also helps a lot in our case since we have too much dummy variables. The test accuracy raised to around 76% when training the random forest model. To fasten the training process, we have also applied Apache Spark due to its fast and efficient processing of large-scale data sets. Perhaps there could be more effort spent on hyperparameters tuning for reaching a more successful model. However, this result is actually the lowest in our more advanced models. This might relate to the fact that our data has wide and sparse features and random forest could only randomly split and pick a few to include in the model

- *XGBoost*

 The XGBoost acquire the highest accuracy of 91%. Build upon a similar idea of bagging models, XGBoost benefits from cross-validation and combination of models to reduce variance. While it trains incrementally with emphasis on the mis-classfied samples, slightly better accuracy is achieved. If we choose not to solve the inbalance class before, the direct application of XGBoost might be a great option since the minority class that are mis-classified would be amplified in the gradient.
 
- *Feedforward Neural Network*
 
 Beyond simple regression and classification methods mentioned before, some neural networks also came to our sights which can potentially capture some relationships within the data that humans couldn't observe. We started from feedforward neural networks by customizing the hidden layers and testing different arrangements of layers. The test accuracy of it is around 88% which is lower than than the XGBoost. The training accuracy keeps growing linearly while our validation curve has arrived at platform.Though it is not technically overfit, but it do suggest us somehow reduce the quick divergence in train and validation accuracy. Thus, the model architecture of the FNN model might need to be reconstructed to decrease its complexity and better fit the situation here. While we build a smaller and simpler FNN model and train the data, it converges quicker but our accuracy is also reduced which might indicate there might be an better architecture of our FNN rather than (relu+linear) to capture more info while not overfits too much. As our FNN is built without cross-validation, it is normal to have certain gap between train performance and test performance.
 
- *Long Short-Term Memory (LSTM) Neural Network*
 
 Another neural network we tried is the long short-term memory (LSTM) neural network. In this neural network, we mainly focused on the effect of review text content, such as comment messages and comment titles, on the review score. Different from other Recurrent Neural Networks, LSTM neural networks can store past information and use it to make predictions. By looking at the RSME it produced, our predictions by this model show a good result while only off around 1 for the review score. The deviation of the prediction might be because this model focuses on the review text itself and we need to acknowledge the existence of the inconsistency between customers' reviews and the scores they give. Despite classifying rating on a scale of 5, we also tried to deduce the score scale to binary (0 and 1) where scores 1, 2, 3 will be 0 and 4, and 5 will be 1 in this model. This binary classification shows an accuracy close to 90% which is higher than the accuracy it has shown while training on a scale of 5. This prediction can provide users with the predicted customers' satisfaction with the shopping experience of the product with relatively high accuracy. Since binary classification is also informative enough to tell if the user is satisfied with the goods or services, the implementation of the binary score scale can be valuable for production as well.

## **Section 8: Potential Next Steps**

Our full dataset for inputs are joined with several seperate dataset which might have different data distributions and behaviors. Though we have take a look at co-linearity and the distribution of data at the EDA stage and transformed and removed few columns, we are not able to look into every columns of data and apply further normalization (in terms of taking log or other transform rather than just offseting by mean and dividing by std) on each. Another problem from joining dataset is that we would encounter some NaN values in our data. Luckily, those rows of NaN is rather small compared to our whole datasets, and dropna won't influence much. And for categorical data, some are also kept and treated as a new type 'NaN'. It is the best choice? For numerical data, we could take the average and fill the NaN, and for the categorical data, we might use majority vote to fill the most frequent data.Further, the knn algorithm could be used to predict the NaN values with that of other data records similar to the missing one. With this unsupervised algorithm introduced in the data process, our result might be improved further. The result might also varies with the machine learning model we used and could take much more time to investigate.

The inbalance of our label classes is another concern. We use combination of groups to deal with it, which do works well. However, there are also other ways to reduce the inbalance. We could resample the groups with higher amount or oversmaple the groups with lower amount. It is hard to tell which is better until each is tried out and this would definetly be on our waitlist for further exploration.

We have spent a lot of on hyperparameter tuning on our randoms forest, xgboost, FNN and LSTM. In the future we would like to achieve this systematically. For example we might try grid search and random search.In grid search, a set of hyperparameters is defined, and a grid is created with all possible combinations of these values. The model is then trained and evaluated using each combination, and the combination that produces the best performance is chosen.In random search, hyperparameter values are chosen at random, and the model is trained and evaluated using these values. 
However, both grid search and random search can be computationally expensive, especially for large datasets and models with many hyperparameters, which is part ofthe reason that we doesn't implemented those.If possible we would like to explore other more complicated methods, such as Bayesian optimization that have been developed to more efficiently tune hyperparameters.

Our dataset also have tons of categorical data included. As for almost every machine learning tools, those 'string' categories doesn't mean anything to the algorithm and we could only transform them into numerical dummy variables for machine to compute. This transform, however, turns several columns of features into more than one hundred of sparse dummy features which drastically increase the dimension of our data. The high dimension with relatively low information would add noise for most machine leanring model, slow down the training and potentially influence the accuracy. One of solution is to combine our knowledge from the EDA stage to split our data into different groups that we want to investigate. For example, we could split orders by the state of customer or seller since we do see a pattern that related to their location in EDA part. Also, we could use the unsupervised clustering algorithms to decide how we split the data with more precision but less interprebility. Those process would leads to different ML models on each groups we splited and challenge us to build a automatic pipline for training, hyperparameter-tuning and visualization our results. Since the order information of transactions is updating all the time and somehow shares a similar schema, we might combine our pipeline with storm to build a real-time predicts for order rating and make interventions for possibly bad rating on time.

## **References:**

1. [spaCy library](https://spacy.io/)
2. [PySpark MLlib](https://spark.apache.org/docs/latest/ml-guide.html)
3. [PyTorch](https://pytorch.org/docs/stable/index.html)
4.[XGBoost Python](https://xgboost.readthedocs.io/en/stable/python/python_intro.html)
"""